<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>不听话的兔子君</title>
  
  <subtitle>那破云的光终会洒落，将有人迎向你的身侧</subtitle>
  <link href="https://naughtyrabbit.github.io/atom.xml" rel="self"/>
  
  <link href="https://naughtyrabbit.github.io/"/>
  <updated>2022-08-21T09:25:14.133Z</updated>
  <id>https://naughtyrabbit.github.io/</id>
  
  <author>
    <name>naughtyrabbit</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>FPGA_动态功能切换_UltraScale 基本 DFX 流程</title>
    <link href="https://naughtyrabbit.github.io/2022/08/20/FPGA-DFX-firstprj/"/>
    <id>https://naughtyrabbit.github.io/2022/08/20/FPGA-DFX-firstprj/</id>
    <published>2022-08-20T08:53:50.000Z</published>
    <updated>2022-08-21T09:25:14.133Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>UltraScale 基本 DFX 流程。目前动态可重构(Partial Reconfiguration) 已经更名为DFX(Dynamic Function eXchange, 动态功能切换)，方便软件工程师也能更容易理解这个Feature.​   </p></blockquote><span id="more"></span><p>&amp;ensp;&amp;ensp;本流程介绍 UltraScale™ 和 UltraScale+™ 器件的基本动态函数交换 (DFX) 流程。首先，您将使用脚本单独综合静态模块和每个可重新配置的设计模块变体。然后在 IDE 中，您将使用 Pblock 约束可重配置模块 (RM) 的位置并实现设计的初始配置。接下来，您将通过锁定设计的静态部分、使用变体更新可重新配置模块以及重新运行实现来实现替代配置。最后，您将验证每个实现的 RM 是否与设计的静态部分兼容，如果兼容，则生成比特流。</p><h3 id="一、解压例程"><a href="#一、解压例程" class="headerlink" title="一、解压例程"></a>一、解压例程</h3><p>&amp;ensp;&amp;ensp;根据ug947文档提示在赛灵思官网下载例程压缩包，定位到 <strong>\led_shift_count_us</strong> 文件夹为<strong>UltraScale</strong>系列例程，在2020.1版本中仅支持vcu118，在2022.1版本中新增了支持。需要注意的是例程是为KCU105, VCU108, KCU116, and VCU118这四块板子定做的。因为反正都没有，不上板的情况下使用2022版本例程文件学习。  </p><h3 id="二、检查脚本"><a href="#二、检查脚本" class="headerlink" title="二、检查脚本"></a>二、检查脚本</h3><p>&amp;ensp;&amp;ensp;首先查看设计档案中提供的脚本。文件 run_dfx.tcl 和 advanced_settings.tcl 位于根级别。 run_dfx.tcl 脚本包含运行 Dynamic Function eXchange 所需的最低设置。 advanced_settings.tcl 包含默认流设置，只能由有经验的用户修改。   </p><h4 id="1-主脚本："><a href="#1-主脚本：" class="headerlink" title="1. 主脚本："></a>1. 主脚本：</h4><p>&amp;ensp;&amp;ensp;在 \led_shift_count_us 中，在文本编辑器中打开 run_dfx.tcl。这是您定义设计参数、设计源和设计结构的主脚本。这是编译完整的 Dynamic Function eXchange 设计时必须修改的唯一文件。在位于 Tcl_HD 子目录的 README.txt 中查找有关 run_dfx.tcl、advanced_settings.tcl 和底层脚本的更多详细信息。文件具体如下：<br><strong>run_dfx.tcl</strong></p><pre><code class="tcl">##################################################################  Minimum settings required to run DFX flow:###  1. Specify flow steps###  2. Define target board###  3. Identify source directories###  4. Define static module###  5. Define RPs, and their RM variants###################################################################flow control (1 = run step , 0 = skip step)set run.topSynth       1 ;#synthesize staticset run.rmSynth        1 ;#synthesize RM variantsset run.dfxImpl        0 ;#implement each static + RM configurationset run.prVerify       0 ;#verify RMs are compatible with staticset run.writeBitstream 0 ;#generate full and partial bitstreams################################################################## Define target demo board### Valid values are kcu105, vcu108, kcu116 and vcu118### Select one only###############################################################set xboard        &quot;kcu105&quot;##################################################################  Run Settings###################################################################Input Directoriesset srcDir     &quot;./Sources&quot;set rtlDir     &quot;$srcDir/hdl&quot;set prjDir     &quot;$srcDir/prj&quot;set xdcDir     &quot;$srcDir/xdc&quot;set coreDir    &quot;$srcDir/cores&quot;set netlistDir &quot;$srcDir/netlist&quot;####Output Directoriesset synthDir  &quot;./Synth&quot;set implDir   &quot;./Implement&quot;set dcpDir    &quot;./Checkpoint&quot;set bitDir    &quot;./Bitstreams&quot;################################################################## Static Module Definition###############################################################set top &quot;top&quot;################################################################## RP &amp; RM Definitions (Repeat for each RP)### 1. Define Reconfigurable Partition (RP) name### 2. Associate Reconfigurable Modules (RMs) to the RP###############################################################set rp1 &quot;shift&quot;set rm_variants($rp1) &quot;shift_right shift_left&quot;set rp2 &quot;count&quot;set rm_variants($rp2) &quot;count_up count_down&quot;########################################################################### RM Configurations (Valid combinations of RM variants)### 1. Define initial configuration: rm_config(initial)### 2. Define additional configurations: rm_config(xyz)########################################################################set module1_variant1 &quot;shift_right&quot;set module2_variant1 &quot;count_up&quot;set rm_config(initial)   &quot;$rp1 $module1_variant1 $rp2 $module2_variant1&quot;set module1_variant2 &quot;shift_left&quot;set module2_variant2 &quot;count_down&quot;set rm_config(reconfig1) &quot;$rp1 $module1_variant2 $rp2 $module2_variant2&quot;########################################################################### Task / flow portion######################################################################### Build the designssource ./advanced_settings.tclsource $tclDir/run.tcl#exit ;#uncomment if running in batch mode</code></pre><p>&amp;ensp;&amp;ensp;请注意此run_dfx.tcl 中的以下详细信息：  </p><ul><li>在Define target demo board 下，您可以选择此设计支持的众多演示板之一。 </li><li>在flow control下，您可以控制运行综合和实现的哪些阶段。在本教程中，脚本只运行合成；实现、验证和比特流生成以交互方式运行。要通过脚本运行这些附加步骤，请将流变量（例如，run.prImpl）设置为 1。 </li><li>Output Directories and Input Directories设置设计源和结果文件的预期文件结构。您必须在此处反映对文件结构的任何更改。 </li><li>Top Definition和RP Module Definitions部分让您可以参考设计每个部分的所有源文件。顶层定义涵盖了静态设计所需的所有资源，包括约束和 IP。 RP Module Definitions 部分对 Reconfigurable Partitions (RP) 执行相同的操作。识别每个 RP 并列出每个 RP 的所有可重构模块 (RM) 变体。 <ul><li>此设计有两个 Reconfigurable Partitions（inst_shift 和 inst_count），每个 RP 有两个模块变体。</li></ul></li><li>Configuration Definition部分定义构成配置的静态和可重新配置模块集。 <ul><li>此设计在主脚本中定义了两种配置：config_shift_right_count_up_implement 和 config_shift_left_count_down_import。 </li><li>您可以通过添加 RM 或组合现有 RM 来创建更多配置。</li></ul></li></ul><h4 id="2-子脚本："><a href="#2-子脚本：" class="headerlink" title="2. 子脚本："></a>2. 子脚本：</h4><p>&amp;ensp;&amp;ensp;在 Tcl_HD 子目录下，存在几个辅助的 Tcl 脚本。这些脚本由 run_dfx.tcl 调用，它们管理 Dynamic Function eXchange 流的特定细节。下面提供了一些关于一些关键 DFX 脚本的详细信息。（官方文档中提示禁止修改这些脚本，只能修改主脚本参数）  </p><ul><li>step.tcl：通过监控检查点来管理设计的当前状态。 </li><li>synthesize.tcl：管理关于综合阶段的所有细节。 </li><li>implement.tcl：管理有关模块实现阶段的所有细节。 </li><li>dfx_utils.tcl：管理有关DFX 设计顶层实现的所有细节。 </li><li>run.tcl：启动综合和实现的实际运行。 </li><li>log_utils.tcl：处理流程中关键点的报告文件创建。  </li><li>其余脚本在这些脚本中提供详细信息（例如其他 *_utils.tcl 脚本）或管理其他分层设计流程（例如 hd_utils.tcl）。</li></ul><h3 id="三、综合设计"><a href="#三、综合设计" class="headerlink" title="三、综合设计"></a>三、综合设计</h3><p>&amp;ensp;&amp;ensp;run_dfx.tcl 脚本自动完成本教程的综合阶段。调用了五次综合迭代，一次用于静态顶层设计，一次用于四个可重配置模块中的每一个。   </p><ol><li>打开 Vivado Tcl shell： <ul><li>在 Windows 上，选择 Xilinx Vivado 桌面图标或开始 → 所有程序 → Xilinx 设计工具 → Vivado 2022.1 → Vivado 2022.1 Tcl Shell。 </li><li>在Linux 上，只需键入vivado -mode tcl。</li></ul></li><li>在 shell 中，导航到 \led_shift_count_us。 </li><li>如果您使用的目标演示板不是 KCU105，请修改 run_dfx.tcl 中的 xboard 变量。有效的替代品是 VCU108、KCU116 和 VCU118 板。 </li><li>输入以下命令运行 run_dfx.tcl 脚本： <blockquote><p>source run_dfx.tcl -notrace   </p></blockquote><p>完成所有五次 Vivado 综合后，Vivado Tcl shell 保持打开状态。您可以在 Synth 子目录中的每个命名文件夹下找到每个模块的日志和报告文件，以及最终检查点。</p></li></ol><blockquote><p>提示：在 \led_shift_count_us 中，创建了多个日志文件：<br>• run.log 显示发布在 Tcl shell 窗口中的摘要<br>• command.log 回显脚本运行的所有单独步骤<br>• critical.log 报告在运行期间产生的所有严重警告运行</p></blockquote><p>&amp;ensp;&amp;ensp;打开tcl控制台，切换到目标目录，注意路径采用 &#x2F; 分隔而不是资源管理器直接复制来的 \ 。执行source run_dfx.tcl -notrace命令，可以看到vivado在进行5次综合。<br><img src="https://m1.im5i.com/2022/08/21/UqUjGB.png" alt="打开tcl控制台"><br>&amp;ensp;&amp;ensp;一段时间后综合完成：<br><img src="https://m1.im5i.com/2022/08/21/UqUTNz.png" alt="综合完成">  </p><h3 id="四、组装和实现设计"><a href="#四、组装和实现设计" class="headerlink" title="四、组装和实现设计"></a>四、组装和实现设计</h3><p>&amp;ensp;&amp;ensp;现在每个模块的综合检查点以及顶部都可用，您可以组装设计。<br>&amp;ensp;&amp;ensp;您将从 Tcl 控制台运行所有流程步骤，但您可以使用 IDE 中的功能（例如布局规划工具）进行交互式事件。</p><ol><li><p>打开 Vivado IDE。您可以通过键入 start_gui 或使用命令 vivado -mode gui 启动 Vivado 从打开的 Tcl shell 打开 IDE。</p></li><li><p>导航到 <del>\led_shift_count_7s</del>(很明显官方文档书写错误，U+系列的应该还是\led_shift_count_us，估计写文档的在复制粘贴 -.- )。如果您还没有。 pwd 命令可以确认这一点。 </p></li><li><p>设置有助于将本文档中的命令复制到 Tcl 控制台的变量。选择您针对此教程的器件和电路板，然后在 Vivado 中应用它们：  </p><pre><code>set part &quot;xcku040-ffva1156-2-e&quot;  set board &quot;kcu105&quot;  set part &quot;xcvu095-ffva2104-2-e&quot;  set board &quot;vcu108&quot;  set part &quot;xcku5p-ffvb676-2-e&quot;  set board &quot;kcu116&quot;  set part &quot;xcvu9p-flga2104-2l-e&quot;  set board &quot;vcu118&quot;  </code></pre></li><li><p>通过在 Tcl 控制台中发出以下命令创建内存中设计： create_project -in_memory -part $part   </p></li><li><p>通过发出以下命令加载静态设计： add_files .&#x2F;Synth&#x2F;Static&#x2F;top_synth.dcp </p></li><li><p>加载通过发出以下命令设置顶层设计约束:  </p><pre><code>add_files ./Sources/xdc/top_io_$board.xdc   set_property USED_IN &#123;implementation&#125; [get_files ./Sources/xdc/top_io_ $board.xdc]</code></pre><p> &amp;ensp;&amp;ensp;选择 top_io_$board 版本的可用的 xdc 文件加载管脚位置和时钟约束，但不包括布局规划信息。 top_$board 版本包括引脚位置、时钟和布局规划约束。  </p></li><li><p>通过发出以下命令为 shift 和 count 函数加载前两个综合检查点：   </p><pre><code>add_files ./Synth/shift_right/shift_synth.dcp   set_property SCOPED_TO_CELLS &#123;inst_shift&#125; [get_files ./Synth/shift_right/shift_synth.dcp]   add_files ./Synth /count_up/count_synth.dcp   set_property SCOPED_TO_CELLS &#123;inst_count&#125; [get_files ./Synth/count_up/count_synth.dcp]  </code></pre><p> &amp;ensp;&amp;ensp;SCOPED_TO_CELLS 属性确保对目标单元进行正确分配。如需了解更多信息，请参阅 Vivado Design Suite 用户指南：使用约束 (UG903) 中的此链接。 </p></li><li><p>使用 link_design 命令将整个设计链接在一起： </p><pre><code>link_design -mode default -reconfig_partitions &#123;inst_shift inst_count&#125; -part $part -top top </code></pre><p> &amp;ensp;&amp;ensp;此时加载了完整的配置，包括静态和可重新配置的逻辑。请注意，当您在非项目模式下工作时，Flow Navigator 窗格不存在。  </p><blockquote><p>提示：通过选择布局 → 布局规划将 IDE 置于布局规划模式。确保“设备”窗口可见。</p></blockquote></li><li><p>保存此初始配置的组装设计状态：  </p><pre><code>write_checkpoint ./Checkpoint/top_link_right_up.dcp</code></pre></li></ol><p>&amp;ensp;&amp;ensp;配置完成后界面如下：<br><img src="https://m1.im5i.com/2022/08/21/UqUhxs.png" alt="综合完成">  </p><h3 id="五、建立设计平面图"><a href="#五、建立设计平面图" class="headerlink" title="五、建立设计平面图"></a>五、建立设计平面图</h3><p>&amp;ensp;&amp;ensp;接下来，创建一个平面图来定义 Dynamic Function eXchange 的区域。  </p><ol><li><p>在 Netlist 窗口中选择 inst_count 实例。右键选择Floorplanning→Draw Pblock，在设备左上角左侧画一个高的窄框。此时，确切的大小和形状并不重要，但请将框保持在时钟区域内。 </p><p>在继续之前，请确保在 Device 窗口中选择了 Pblock。  </p><p>尽管此 Reconfigurable Module 仅需要 CLB 资源，但如果框包含这些类型，还包括 RAMB18、RAMB36 或 DSP48 资源。这允许这些块类型的路由资源包含在可重新配置区域中。如果需要，可以使用 Pblock Properties 窗口的 General 视图添加这些。 Statistics 视图显示当前加载的 Reconfigurable Module 的资源需求。</p><p><img src="https://m1.im5i.com/2022/08/21/UqUkqo.png" alt="draw_pblock"><br><img src="https://m1.im5i.com/2022/08/21/UqUC6W.png" alt="pblock"></p></li><li><p>对 inst_shift 实例重复上一步，这次针对第一个下方的时钟区域。此 Reconfigurable Module 包含 Block RAM 实例，因此必须包含资源类型。如果省略，统计视图中的 RAMB 详细信息将显示为红色。<br><img src="https://m1.im5i.com/2022/08/21/UqUcHx.png" alt="重复画一个pblock"><br><img src="https://m1.im5i.com/2022/08/21/UqU8TQ.png" alt="包含资源类型和统计数据"></p></li><li><p>通过选择 Reports → Report DRC 运行 Dynamic Function eXchange 设计规则检查。您可以取消选中所有规则，然后选中 Dynamic Function eXchange 以使此报告严格关注 DFX DRC。<br><img src="https://m1.im5i.com/2022/08/21/UqUFeq.png" alt="选择规则"></p><p>只要 inst_shift Pblock 包含 RAMB18 和 RAMB36 资源，就不应报告 DRC 错误。可能仍会报告建议消息，尤其是当 Pblock 位于设备边缘附近时。请注意，对于两个 Pblock，SNAPPING_MODE 都设置为 ON，如 Pblock Properties 窗口的 Properties 视图中所述。鉴于此架构中可编程单元的精细粒度，所有 UltraScale 和 UltraScale+ 设备始终启用此功能。<br><img src="https://m1.im5i.com/2022/08/21/UqUV3D.png" alt="选择规则"></p></li><li><p>保存这些 Pblock 和相关属性： </p><pre><code>write_xdc ./Sources/xdc/top_all.xdc</code></pre><p> 这会导出设计中的所有当前约束，包括之前从 top_io_$board.xdc 导入的约束。这些约束可以在它们自己的 XDC 文件中管理或在运行脚本中管理（通常使用 HD.RECONFIGURABLE 完成）。<br> 或者，可以单独提取和管理 Pblock 约束本身。 Tcl proc 可用于帮助执行此任务。  </p><p> a. 首先获取在 Tcl 实用程序文件之一中找到的 proc：  </p><pre><code>source ./Tcl_HD/hd_utils.tcl</code></pre><p> b. 然后使用 export_pblocks proc 写出此约束信息： </p><pre><code>export_pblocks -file ./Sources/xdc/pblocks.xdc</code></pre><p> &amp;ensp;&amp;ensp;这将为设计中的两个 Pblock 写入 Pblock 约束信息。如果需要，使用 pblocks 选项仅选择一个。</p></li></ol><h3 id="六、实现第一个配置"><a href="#六、实现第一个配置" class="headerlink" title="六、实现第一个配置"></a>六、实现第一个配置</h3><p>&amp;ensp;&amp;ensp;在此步骤中，您对设计进行布局和布线，并准备设计的静态部分，以便与新的可重配置模块一起重复使用。</p><h4 id="设计实现"><a href="#设计实现" class="headerlink" title="设计实现"></a>设计实现</h4><ol><li>通过发出以下命令来优化、布局和布线设计：</li></ol><pre><code>opt_designplace_designroute_design</code></pre><p>   在 place_design 和 route_design 之后，在 Device 视图中检查设计的状态（参见下图）。在 place_design 之后需要注意的一件事是引入了分区引脚。这些是静态和可重新配置逻辑之间的物理接口点。它们是互连块中的锚点，可重配置模块的每个 I&#x2F;O 都必须通过这些锚点进行路由。它们在放置的设计视图中显示为白框。对于 pblock_shift，它们出现在该 Pblock 的顶部，因为到静态的连接就在设备该区域的 Pblock 之外。对于 Pblock_count，它们出现在用户定义的区域之外，因为 SNAPPING_MODE 垂直收集了更多要添加到 Reconfigurable Partition 的帧。<br>   <img src="https://m1.im5i.com/2022/08/21/UqU5py.png" alt="优化和布局布线之后">  </p><ol start="2"><li><p>要在 GUI 中轻松找到这些分区引脚：<br>a. 在 Netlist 窗格中选择 Reconfigurable Module（例如，inst_shift）<br>b. 在Cell Properties窗格中选择Cell Pins选项卡。  </p></li><li><p>选择任何引脚以突出显示它，或使用 Ctrl+A 全选。后者的 Tcl 等效项是：  </p><pre><code>select_objects [get_pins inst_shift/*]</code></pre><pre><code>![分区引脚](https://m1.im5i.com/2022/08/21/UqUGbh.png)</code></pre></li><li><p>使用 Routing Resources 工具栏按钮在抽象和实际路由信息之间切换，并更改路由资源本身的可见性。此时设计中的所有网络都已完全布线。<br>   <img src="https://m1.im5i.com/2022/08/21/UqUbNX.png" alt="分区引脚"></p></li></ol><h4 id="保存结果"><a href="#保存结果" class="headerlink" title="保存结果"></a>保存结果</h4><ol><li><p>通过发出以下命令保存完整的设计检查点并创建报告文件： </p><pre><code>write_checkpoint -force ImplementConfig_shift_right_count_up_implement/top_route_design.dcpreport_utilization -file Implement/Config_shift_right_count_up_implement/top_utilization.rptreport_timing_summary -file Implement/Config_shift_right_count_up_implement/top_timing_summary.rpt</code></pre></li><li><p>[可选] 通过发出以下两个命令为每个可重新配置模块保存检查点：  </p><pre><code>write_checkpoint -force -cell inst_shift Checkpoint/shift_right_route_design.dcpwrite_checkpoint -force -cell inst_count Checkpoint/count_up_route_design.dcp</code></pre><blockquote><p>提示：运行 run_dfx.tcl 以批处理模式处理整个设计时，会在流程的每个步骤中创建设计检查点、日志文件和报告文件。  </p></blockquote><p> 至此，您已经创建了一个完全实现的 Dynamic Function eXchange 设计，您可以从中生成全部和部分比特流。此配置的静态部分用于所有后续配置。要隔离静态设计，请移除当前的 Reconfigurable Modules。确保启用路由资源，并放大到带有分区引脚的互连块。  </p></li><li><p>通过发出以下命令清除 Reconfigurable Module 逻辑：  </p><pre><code>update_design -cell inst_shift -black_boxupdate_design -cell inst_count -black_box</code></pre><p> 发出这些命令会导致许多设计更改，如下图所示：<br> • 全布线网络（绿色）的数量减少。<br> • inst_shift 和inst_count 现在在网表视图中显示为空。<br> <img src="https://m1.im5i.com/2022/08/21/UqU2xf.png" alt="全布线网络（绿色）的数量减少"><br> <img src="https://m1.im5i.com/2022/08/21/UqU78M.png" alt="全布线网络（绿色）的数量减少"></p></li></ol><h4 id="关闭项目"><a href="#关闭项目" class="headerlink" title="关闭项目"></a>关闭项目</h4><ol><li><p>发出以下命令以锁定所有布局和布线：</p><pre><code>lock_design -level routing</code></pre><p> 因为在 lock_design 命令中没有识别出单元，所以内存中的整个设计（当前包括带有黑盒的静态设计）都会受到影响。所有布线网络现在都显示为锁定，如图 第7节图 中的虚线所示。所有放置的组件都从蓝色变为橙色，表明它们也被锁定。  </p></li><li><p>发出以下命令以写出（write out）剩余的仅静态检查点：  </p><pre><code>write_checkpoint -force Checkpoint/static_route_design.dcp</code></pre><p> 此仅静态检查点用于将来的配置。  </p></li><li><p>在继续下一个配置之前关闭此设计：  </p><pre><code>close_project</code></pre></li></ol><h3 id="七、实施第二个配置"><a href="#七、实施第二个配置" class="headerlink" title="七、实施第二个配置"></a>七、实施第二个配置</h3><p>&amp;ensp;&amp;ensp;现在静态设计结果已建立并锁定，您可以将其用作实现更多可重构模块的上下文。</p><p>参考：ug947-2022.1</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;UltraScale 基本 DFX 流程。目前动态可重构(Partial Reconfiguration) 已经更名为DFX(Dynamic Function eXchange, 动态功能切换)，方便软件工程师也能更容易理解这个Feature.​   &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="DFX" scheme="https://naughtyrabbit.github.io/categories/DFX/"/>
    
    
    <category term="FPGA" scheme="https://naughtyrabbit.github.io/tags/FPGA/"/>
    
    <category term="部分可重构" scheme="https://naughtyrabbit.github.io/tags/%E9%83%A8%E5%88%86%E5%8F%AF%E9%87%8D%E6%9E%84/"/>
    
    <category term="DFX" scheme="https://naughtyrabbit.github.io/tags/DFX/"/>
    
  </entry>
  
  <entry>
    <title>FPGA_PR_1app</title>
    <link href="https://naughtyrabbit.github.io/2022/08/20/FPGA-PR-1app/"/>
    <id>https://naughtyrabbit.github.io/2022/08/20/FPGA-PR-1app/</id>
    <published>2022-08-20T03:15:36.000Z</published>
    <updated>2022-08-20T03:16:25.758Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要是…   </p></blockquote><span id="more"></span><p>&amp;ensp;&amp;ensp;正文 </p><h3 id="一、一级标题"><a href="#一、一级标题" class="headerlink" title="一、一级标题"></a>一、一级标题</h3>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要是…   &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>FPGA_部分可重构技术_0介绍</title>
    <link href="https://naughtyrabbit.github.io/2022/08/19/FPGA_PR_0/"/>
    <id>https://naughtyrabbit.github.io/2022/08/19/FPGA_PR_0/</id>
    <published>2022-08-19T05:27:04.000Z</published>
    <updated>2022-08-20T03:14:29.814Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要是FPGA中部分可重构技术学习笔记，第0篇。内容主要参考赛灵思官方文档ug909.ug947等，部分翻译可能存在问题，还请指正。这一部分主要为Partial Reconfiguration（PR）的介绍    </p></blockquote><span id="more"></span><p>&amp;ensp;&amp;ensp;在减少尺寸、重量、功率和成本的同时，部分可重构技术使得一些新的FPGA设计成为可能。  </p><h3 id="一、专业名词"><a href="#一、专业名词" class="headerlink" title="一、专业名词"></a>一、专业名词</h3><ol><li>自底向上综合（Bottom-Up Synthesis）<br>&amp;ensp;&amp;ensp;在Vivado中，自底向上综合指的是脱离上下文的综合（<strong>out-of-context synthesis</strong>）.OOC综合为每个OOC模块生成一个单独的网表(或DCP)，并需要进行部分重构，以确保没有在模块边界上进行优化。在OOC综合中，顶层（或静态）逻辑与每个OOC模块的黑盒模块定义一起合成。</li><li>配置（Configuration）<br>&amp;ensp;&amp;ensp;配置是一个完整的设计，对于每个可重构分区都有一个可重构模块。在部分重新配置FPGA项目中可能有许多配置。每个配置都会生成一个完整的BIT文件，以及为每个可重构的模块(RM)生成一个部分的BIT文件。</li><li>配置帧（Configuration Frame）<br>&amp;ensp;&amp;ensp;配置帧是FPGA配置内存空间中最小的可寻址段。可重构的框架是由这些最低级别元素的离散数构建的。在Xilinx设备中，基本可重构帧是一个元素(CLB，RAM，DSP)宽由一个时钟区域高。这些帧中的资源数量因设备系列而异。</li><li>内部配置访问端口（Internal Configuration Access Port）<br>&amp;ensp;&amp;ensp;内部配置访问端口（ICAP）本质上是SelectMAP接口的内部版本</li><li>媒体配置访问端口（Media Configuration Access Port）<br>&amp;ensp;&amp;ensp;MCAP是从每个 UltraScale设备的一个特定的PCIe®块到ICAP的专用链路。在配置Xilinx PCIe IP时，可以启用此入口点。</li><li>部分可重构（Partial Reconfiguration）<br>&amp;ensp;&amp;ensp;部分重构是通过下载部分比特流来修改操作FPGA设计中的逻辑子集</li><li>分区（Partition）<br>&amp;ensp;&amp;ensp;分区是设计的一个逻辑部分，用户在层次边界上定义，以考虑设计重用。分区要么作为新的实现，要么从以前的实现中保留。保留的分区不仅维护相同的功能，而且还维护相同的实现</li><li>分区定义（Partition Definition）<br>&amp;ensp;&amp;ensp;这是一个仅在项目流中使用的术语。分区定义定义了一组与模块实例（或可重构分区）关联的可重构模块。PD应用于模块的所有实例，并且不能与模块实例的子集相关联。</li><li>分区引脚（Partition Pin）<br>&amp;ensp;&amp;ensp;分区引脚是静态逻辑和可重构逻辑之间的逻辑和物理连接。工具会自动创建、放置和管理分区引脚。</li><li>处理器配置访问端口（Processor Configuration Access Port）<br>&amp;ensp;&amp;ensp;处理器配置访问端口(PCAP)类似于内部配置访问端口(ICAP)，是用于配置Zynq-7000 AP SoC设备的主端口。</li><li>可编程部件（Programmable Unit）<br>&amp;ensp;&amp;ensp;在UltraScale结构中，这是重新配置所需的最小资源。PU的大小因资源类型而异。因为相邻的站点在UltraScale体系结构中共享一个路由资源（或互连贴图），所以PU是根据成对来定义的。</li><li>可重构帧（Reconfigurable Frame）<br>&amp;ensp;&amp;ensp;可重构帧（在本指南中除“配置帧”之外的所有参考文献中）表示FPGA中最小的可重构区域。可重构帧的比特流大小取决于帧中所包含的逻辑类型。</li><li>可重构逻辑（Reconfigurable Logic）<br>&amp;ensp;&amp;ensp;可重构逻辑是作为可重构模块的一部分的任何逻辑元素。当加载部分BIT文件时，会修改这些逻辑元素。可以重新配置逻辑组件的许多类型，如LUTs、flip-flops、块RAM和DSP块。</li><li>可重构模块（Reconfigurable Module）<br>&amp;ensp;&amp;ensp;可重构模块(RM)是在可重构分区中实现的网络列表或HDL描述。对于一个可重构的分区，存在多个RMs。</li><li>可重构分区（Reconfigurable Partition）<br>&amp;ensp;&amp;ensp;可重构分区(RP)是实例上设置的属性，它将实例定义为可重构。可重构分区是在其中实现不同的可重构模块的层次结构级别。Tcl命令，如opt_design,<br>place_design和route_design检测实例上的HD.RECONFIGURABLE 属性，并正确地处理它。</li><li>静态逻辑（Static Logic）<br>&amp;ensp;&amp;ensp;静态逻辑是不属于RP的任何逻辑元素。逻辑元素永远不会部分重新配置，并且在重新配置rp时始终处于活动状态。静态逻辑也被称为顶级逻辑。</li><li>静态设计（Static Design）<br>&amp;ensp;&amp;ensp;静态设计是设计中在部分重新配置过程中不会改变的部分。静态设计包括顶层模块和所有未定义为可重构模块的模块。静态设计采用静态逻辑和静态路由来构建</li></ol><h3 id="二、设计注意事项"><a href="#二、设计注意事项" class="headerlink" title="二、设计注意事项"></a>二、设计注意事项</h3><ol><li>设计指南<ul><li>需要平面规划来定义每个元素类型的可重新配置区域。</li><li>自下而上&#x2F;OOC 综合（创建多个网表&#x2F;DCP 文件）和可重配置模块网表文件的管理是用户的责任。</li><li>已经建立了一套独特的设计规则检查 (DRC)，以帮助确保成功完成设计。</li><li>PR 设计必须考虑部分重配置的启动以及部分 BIT 文件的交付，无论是在 FPGA 内还是作为系统设计的一部分。</li><li>Vivado 设计套件包括对部分重配置控制器 IP 的支持。这种可定制的 IP 可管理任何 Xilinx 器件中部分重配置的核心任务。内核从硬件或软件接收触发器，管理握手和解耦任务，从内存位置获取部分比特流，并将它们传递给 ICAP。</li><li>可重配置分区必须包含所有引脚的超集，供为分区实现的各种可重配置模块使用。如果 RM 使用来自另一个 RM 的不同输入或输出，则生成的 RM 输入或输出可能不会连接到 RM 内部。这些工具通过在 RM 中为所有未使用的输入和输出插入一个 LUT1 缓冲区来处理此问题。输出 LUT1 绑定到一个常数值，常数的值可以由未使用的输出引脚上的 HD.PARTPIN_TIEOFF 属性控制。</li><li>对于用户复位信号，确定 RM 内部的逻辑是电平敏感还是边沿敏感。如果复位电路是边沿敏感的（因为它可能在某些 IP，如 FIFO 中），则在重新配置完成之前不应应用 RM 复位。</li></ul></li><li>设计标准  <ul><li>对于 UltraScale 和 UltraScale+ 器件，可重新配置的组件类型列表更为广泛：<br><img src="https://m1.im5i.com/2022/08/20/UqDWRK.png" alt="U+系列支持PR组件"></li><li>可重新配置模块必须进行初始化，以确保重新配置后的可预测启动条件。对于 7 系列以外的所有设备，PR 完成后会自动应用 GSR。对于 7 系列设备，在满足 Pblock 要求后，可以使用 RESET_AFTER_RECONFIG Pblock 属性打开 GSR。</li><li>强烈建议使用去耦逻辑，以便在部分重配置操作期间将可重配置区域与设计的静态部分断开。</li><li>实现工具禁止跨 PR 边界进行优化。 PR 设计中的 WNS 路径通常是跨越 RP 边界的高扇出控制&#x2F;复位信号。避免高扇出信号穿过 RP 边界，因为驱动器无法复制。为了使工具具有最大的优化&#x2F;复制灵活性，请考虑以下几点：<ul><li>对于 RP 的输入，使穿过 RP 边界的信号成为单个扇出网络，并在扇出之前将信号寄存在 RM 内。这可以根据需要在 RM 内复制（或放在全局资源上）。</li><li>对于输出，再次使穿过 PR 边界的信号成为单个扇出网络。在扇出之前将信号注册为静态以进行复制&#x2F;优化。</li></ul></li><li>对于具有多个 RP 的设计，赛灵思建议不要在两个 RP 之间建立直接连接。这包括通过异步静态逻辑（未在静态中注册）的连接。如果两个 RP 之间存在直接连接，则必须在静态时序分析中验证所有可能的配置，以确保跨这些接口满足时序要求。这可以针对完全由单个用户拥有和维护的封闭系统完成，但对于由多个用户开发不同 RM 的设计可能无法验证。在静态中添加同步端点可确保在任何配置上始终满足时序，只要实现 RM 的配置满足时序。<blockquote><p>&amp;ensp;&amp;ensp;部分重配置是赛灵思器件中的一项强大功能，了解芯片和软件的功能对于该技术的成功至关重要。虽然在开发过程中必须承认和考虑权衡，但总体结果是更灵活地实现您的 FPGA 设计。</p></blockquote></li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要是FPGA中部分可重构技术学习笔记，第0篇。内容主要参考赛灵思官方文档ug909.ug947等，部分翻译可能存在问题，还请指正。这一部分主要为Partial Reconfiguration（PR）的介绍    &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="部分可重构" scheme="https://naughtyrabbit.github.io/categories/%E9%83%A8%E5%88%86%E5%8F%AF%E9%87%8D%E6%9E%84/"/>
    
    
    <category term="FPGA" scheme="https://naughtyrabbit.github.io/tags/FPGA/"/>
    
    <category term="部分可重构" scheme="https://naughtyrabbit.github.io/tags/%E9%83%A8%E5%88%86%E5%8F%AF%E9%87%8D%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Vitis-AI 编译使用minist解读</title>
    <link href="https://naughtyrabbit.github.io/2022/03/23/vitisai-minist/"/>
    <id>https://naughtyrabbit.github.io/2022/03/23/vitisai-minist/</id>
    <published>2022-03-23T04:27:13.000Z</published>
    <updated>2022-03-28T15:23:38.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>由于TVM框架目前未找到合适的方法部署神经网络（pytorch框架）在FPGA上，故想要单独尝试Vitis-AI编译网络流程。一下是官方的以手写数字识别的vitis-ai pytorch流的指导。记录执行过程并分析。</p></blockquote><span id="more"></span><p>&amp;ensp;&amp;ensp;整体的flow如下图所示：<br><img src="https://s1.ax1x.com/2022/03/23/qlLOqs.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;首先利用数据集训练得到模型文件，然后用校准数据和模型共同输入到量化器中，然后编译生成的.xmodel得到可以再FPGA上运行的.xmodel文件。最后在目标FPGA平台上使用生成的.xmodel模型文件。</p><h3 id="一、训练"><a href="#一、训练" class="headerlink" title="一、训练"></a>一、训练</h3><p>&amp;ensp;&amp;ensp;该脚本将执行 CNN 的训练，并将训练的浮点模型另存为 .&#x2F;build&#x2F;float_model 文件夹中的 .pth 文件。即f_model.pth</p><pre><code class="bash"># foldersexport BUILD=./buildexport LOG=$&#123;BUILD&#125;/logsmkdir -p $&#123;LOG&#125;# run trainingpython -u train.py -d $&#123;BUILD&#125; 2&gt;&amp;1 | tee $&#123;LOG&#125;/train.log</code></pre><h3 id="二、量化"><a href="#二、量化" class="headerlink" title="二、量化"></a>二、量化</h3><p>&amp;ensp;&amp;ensp;Xilinx DPU 系列 ML 加速器执行的模型和网络的参数采用整数格式，因此我们必须将经过训练的浮点检查点转换为定点整数检查点 - 此过程称为量化。<br>&amp;ensp;&amp;ensp;量化完成后，可以在 .&#x2F;build&#x2F;quant_model 文件夹中找到量化模型。  </p><pre><code class="bash"># quantize &amp; export quantized modelpython -u quantize.py -d $&#123;BUILD&#125; --quant_mode calib 2&gt;&amp;1 | tee $&#123;LOG&#125;/quant_calib.logpython -u quantize.py -d $&#123;BUILD&#125; --quant_mode test  2&gt;&amp;1 | tee $&#123;LOG&#125;/quant_test.log</code></pre><p>&amp;ensp;&amp;ensp;量化模式分为量化和评估，可以在评估中看到量化对模型推理准确性的影响。事实上，不管是test还是calib模式，都会对量化的模型进行评估，唯一不同的是，calib模式会导出量化参数，而test模式会导出量化后的模型。</p><pre><code class="python"># export config  if quant_mode == &#39;calib&#39;:    quantizer.export_quant_config()  if quant_mode == &#39;test&#39;:    quantizer.export_xmodel(deploy_check=False, output_dir=quant_model)</code></pre><p>&amp;ensp;&amp;ensp;然而，比较令人费解的是，两次执行改脚本的量化评估结果却不同：其次是量化之后的两次评估结果都比训练3轮的评估结果要好。可能是因为改模型网络以及输入数据比较简单。<br><img src="https://s1.ax1x.com/2022/03/23/q1aAoT.png" alt="图片描述"><br>———-划重点———————————————<br>&amp;ensp;&amp;ensp;另外，亲测pytorch1.7版本（默认镜像是1.4版本）在导入会报如下错误：</p><pre><code>[VAIQ_ERROR]: /opt/vitis_ai/conda/envs/vitis-ai-pytorch/lib/python3.6/site-packages/pytorch_nndct/nn/_kernels.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceIN3c108BFloat16EEEPKNS_6detail12TypeMetaDataEv</code></pre><p><img src="https://s1.ax1x.com/2022/03/23/q16juF.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;该Bug在vitis-ai用户指南中提到，解决方法是先导入pytorch_nndct，后导入torch。似乎没用。<br>&amp;ensp;&amp;ensp;按照官方的&#x2F;opt&#x2F;viitis_ai&#x2F;scripts&#x2F;replace_pytorch.sh，脚本生成新的pytorch环境我不知道为啥还会有问题，按照脚本在原来的基础上删了原来的torch以及vai_q_pytorch，用1.7的torch环境编译了vai_q_pytorch，之后可以正常使用1.7版本torch进行量化</p><h3 id="三、编译"><a href="#三、编译" class="headerlink" title="三、编译"></a>三、编译</h3><p>&amp;ensp;&amp;ensp;编译后的模型会保存在 .&#x2F;build&#x2F;compiled_model 文件夹。并以目标命名。</p><pre><code class="bash"># compile for target boardssource compile.sh zcu102 $&#123;BUILD&#125; $&#123;LOG&#125;# make target folderspython -u target.py --target zcu102 -d $&#123;BUILD&#125; 2&gt;&amp;1 | tee $&#123;LOG&#125;/target_zcu102.log</code></pre><p>&amp;ensp;&amp;ensp;下面是compile.sh：</p><pre><code class="bash">if [ $1 = zcu102 ]; then      ARCH=/opt/vitis_ai/compiler/arch/DPUCZDX8G/ZCU102/arch.json      TARGET=zcu102      echo &quot;-----------------------------------------&quot;      echo &quot;COMPILING MODEL FOR ZCU102..&quot;      echo &quot;-----------------------------------------&quot;  </code></pre><h3 id="四、导出"><a href="#四、导出" class="headerlink" title="四、导出"></a>四、导出</h3><p>&amp;ensp;&amp;ensp;该脚本将执行以下操作：target.py  </p><ol><li>创建一个名为 .&#x2F;build&#x2F;target_<board_name> 的文件夹。</li><li>将相应的已编译模型复制到 .&#x2F;build&#x2F;target_<board_name> 文件夹。</li><li>将 Python 应用程序代码复制到 .&#x2F;build&#x2F;target_<board_name> 文件夹。</li><li>将 MNIST 测试数据集转换为 PNG 图像文件。<br>图像数由命令行参数设置，该参数默认为 10000。–num_images</li></ol><pre><code class="bash"># compile for target boardssource compile.sh zcu102 $&#123;BUILD&#125; $&#123;LOG&#125;# make target folderspython -u target.py --target zcu102 -d $&#123;BUILD&#125; 2&gt;&amp;1 | tee $&#123;LOG&#125;/target_zcu102.log</code></pre>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;由于TVM框架目前未找到合适的方法部署神经网络（pytorch框架）在FPGA上，故想要单独尝试Vitis-AI编译网络流程。一下是官方的以手写数字识别的vitis-ai pytorch流的指导。记录执行过程并分析。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="Vitis-AI" scheme="https://naughtyrabbit.github.io/categories/Vitis-AI/"/>
    
    
    <category term="Vitis-AI" scheme="https://naughtyrabbit.github.io/tags/Vitis-AI/"/>
    
  </entry>
  
  <entry>
    <title>TVM编译模型（yolov3_tf的例子解读）</title>
    <link href="https://naughtyrabbit.github.io/2022/03/18/TVM-Cperyolov3-tf/"/>
    <id>https://naughtyrabbit.github.io/2022/03/18/TVM-Cperyolov3-tf/</id>
    <published>2022-03-18T14:58:20.000Z</published>
    <updated>2022-03-20T14:54:06.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录一下TVM编译yolov3_tf模型的例子，FPGA开发板：zcu102  </p></blockquote><span id="more"></span><h3 id="一、主机前置环境安装"><a href="#一、主机前置环境安装" class="headerlink" title="一、主机前置环境安装"></a>一、主机前置环境安装</h3><p>&amp;ensp;&amp;ensp;即setup_custom_yolov3.sh这个脚本，下面是详细内容，部分文件以及git仓库是我预先下载好的，以cp取代下载。下面的脚本主要就是：下载了yolov3_coco.tar.gz（即checkpoint文件），解压至checkpoint文件夹。然后利用convert_weight.py将相关权重文件转换为pb格式。使用freeze_graph.py将模型文件和权重数据整合在一起并去除无关的op。最后将整合的tf模型转换为ONNX格式（如代码块后的图所示）</p><pre><code class="sh">. $VAI_ROOT/conda/etc/profile.d/conda.sh  conda activate vitis-ai-tensorflow/opt/vitis_ai/conda/envs/vitis-ai-tensorflow/bin/python3.6 -m pip install --upgrade pip# 加了个换源操作，方便后面pip安装pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simplepip install pydot==1.4.1if [ ! -d &quot;/tmp/tensorflow-yolov3&quot; ]; then    cp -rf tensorflow-yolov3 /tmp/    cp yolov3_coco.tar.gz /tmp/    cd /tmp/    # git clone https://github.com/YunYang1994/tensorflow-yolov3     # git clone https://gitee.com/mirrors_YunYang1994/tensorflow-yolov3    cd tensorflow-yolov3    pip install easydict --user     cd checkpoint     # wget https://github.com/YunYang1994/tensorflow-yolov3/releases/download/v1.0/yolov3_coco.tar.gz     #wget https://a.y8j5.top/s/A58q3fK     #mv A58q3fK yolov3_coco.tar.gz    cp /tmp/yolov3_coco.tar.gz .    tar -xvf yolov3_coco.tar.gz    cd ..     python convert_weight.py    python freeze_graph.py    sed -i &#39;s/.\//\/tmp\/tensorflow-yolov3\//&#39; ./core/config.py    # CONVERT TENSORFLOW MODEL TO ONNX    pip install numpy==1.16.6 --user    pip install onnx --user     # cd &quot;$&#123;TVM_VAI_HOME&#125;&quot;/tensorflow-yolov3    # git clone https://github.com/onnx/tensorflow-onnx.git    cd tensorflow-onnx &amp;&amp; python setup.py install --user &amp;&amp; cd ..    python3 -m tf2onnx.convert --input ./yolov3_coco.pb --inputs input/input_data:0[1,320,320,3] --outputs pred_sbbox/concat_2:0 --output tf_yolov3_converted.onnxfi</code></pre><p><img src="https://s1.ax1x.com/2022/03/20/qVLwJ1.png" alt="图片描述">  </p><h3 id="二、编译模型"><a href="#二、编译模型" class="headerlink" title="二、编译模型"></a>二、编译模型</h3><p>&amp;ensp;&amp;ensp;模型的编译部分其实和之前的mxnet的编译类似，相同的地方不在重复。<a href="https://github.com/Xilinx/Vitis-AI/blob/master/external/tvm/examples/external_yolov3_tutorial.ipynb">https://github.com/Xilinx/Vitis-AI/blob/master/external/tvm/examples/external_yolov3_tutorial.ipynb</a>  官方的jupyter book有详尽的步骤，但是在jupyter上容易达成kernel died。因此，转换为一个python脚本在本地命令行执行（内存占用极大）。下面的各个代码块即是完整的py脚本，连起来可以直接运行。  </p><h4 id="1-import"><a href="#1-import" class="headerlink" title="1. import"></a>1. import</h4><pre><code class="python">&quot;&quot;&quot;-------------------------------------------------Import packages-------------------------------------------------&quot;&quot;&quot;import numpy as npimport os, sysimport os.path# 注意到这里导入了tfimport tensorflow as tffrom pathlib import Path# pyxirimport pyxirimport pyxir.contrib.target.DPUCADF8H# tvm, relayimport tvmfrom tvm import tefrom tvm import contribimport tvm.relay as relay# BYOCfrom tvm.relay import transformfrom tvm.contrib import utils, graph_executorfrom tvm.contrib.target import vitis_aifrom tvm.relay.build_module import bind_params_by_namefrom tvm.relay.op.contrib.vitis_ai import annotation# Tensorflow utility functionsimport tvm.relay.testing.tf as tf_testingfrom tvm.contrib.download import download_testdatafrom tvm.relay.op.contrib.vitis_ai import partition_for_vitis_aiimport cv2try:    tf_compat_v1 = tf.compat.v1except ImportError:    tf_compat_v1 = tf</code></pre><h4 id="2-yolov3相关预处理函数"><a href="#2-yolov3相关预处理函数" class="headerlink" title="2. yolov3相关预处理函数"></a>2. yolov3相关预处理函数</h4><p>&amp;ensp;&amp;ensp;这部分其实在编译模型阶段没有很重要，甚至可以不需要，只是在编译生成模型的时候需要得到mod, params &#x3D; relay.frontend.from_onnx(onnx_model, shape_dict)中的shape_dict，这个步骤在知道网络输入格式后甚至可以直接设置为指定的，比如yolo+csl中[1024*1024].</p><pre><code class="python">-------------------------------------------------Define preprocessing functions定义了一些有关yolov3_tf模型的预处理函数-------------------------------------------------&quot;&quot;&quot;def preprocessing(image):    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)    ih, iw    = (320,320)    h,  w, _  = image.shape    scale = min(iw/w, ih/h)    nw, nh  = int(scale * w), int(scale * h)    image_resized = cv2.resize(image, (nw, nh))    image_padded = np.full(shape=[ih, iw, 3], fill_value=128.0)    dw, dh = (iw - nw) // 2, (ih-nh) // 2    image_padded[dh:nh+dh, dw:nw+dw, :] = image_resized    image_padded = image_padded / 255.    return image_padded   def transform_image(image):    image = np.array(image)[np.newaxis, :]    return image</code></pre><h4 id="3-准备输入，声明DPU目标"><a href="#3-准备输入，声明DPU目标" class="headerlink" title="3. 准备输入，声明DPU目标"></a>3. 准备输入，声明DPU目标</h4><p>&amp;ensp;&amp;ensp;这里的input_name可以在 <a href="https://netron.app/">https://netron.app/</a> 网站上打开ONNX模型查看，如图（yolov5+csl的转ONNX模型）：<br><img src="https://s1.ax1x.com/2022/03/20/qe2ne0.png" alt="图片描述">  </p><pre><code class="python">&quot;&quot;&quot;-------------------------------------------------Prepare input and specify the Vitis DPU target-------------------------------------------------&quot;&quot;&quot;from PIL import Imageimg_path = &quot;/tmp/tensorflow-yolov3/docs/images/road.jpeg&quot;original_image = cv2.imread(img_path)image = preprocessing(original_image) image = np.array(image)[np.newaxis, :]# 这里的image.shape是[1, 320, 320, 3]和onnx打开模型所观察到的一致v_target_value = &quot;DPUCZDX8G-zcu102&quot;print(&quot;Vitis Target: &quot;, v_target_value)input_name     = &#39;input/input_data&#39;shape_dict     = &#123;input_name: image.shape&#125;postprocessing = []vitis_target   = v_target_valuetvm_target     = &#39;llvm&#39;lib_kwargs     = &#123;&#125;</code></pre><h4 id="4-导入模型"><a href="#4-导入模型" class="headerlink" title="4. 导入模型"></a>4. 导入模型</h4><pre><code class="python">&quot;&quot;&quot;-------------------------------------------------Import the model to TVM-------------------------------------------------&quot;&quot;&quot;framework_value = &quot;ONNX&quot;print (&quot;Framework: &quot;, framework_value)if framework_value == &quot;TF&quot;:    model_path = &quot;/tmp/tensorflow-yolov3/yolov3_coco.pb&quot;    with tf_compat_v1.gfile.GFile(model_path, &quot;rb&quot;) as f:        graph_def = tf_compat_v1.GraphDef()        graph_def.ParseFromString(f.read())        graph = tf.import_graph_def(graph_def, name=&quot;&quot;)        # Call the utility to import the graph definition into default graph.        graph_def = tf_testing.ProcessGraphDefParam(graph_def)        # Add shapes to the graph.        with tf_compat_v1.Session() as sess:            graph_def = tf_testing.AddShapesToGraphDef(sess, &quot;pred_sbbox/concat_2&quot;)    mod, params = relay.frontend.from_tensorflow(graph_def, shape=shape_dict)    print(&quot;Tensorflow protobuf imported to relay frontend.&quot;)    else:    import onnx    input_name     = &#39;input/input_data:0&#39;    shape_dict     = &#123;input_name: image.shape&#125;    model_path = &quot;/tmp/tensorflow-yolov3/tf_yolov3_converted.onnx&quot;    onnx_model = onnx.load(model_path)    mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)</code></pre><h4 id="5-Partitioning-the-model"><a href="#5-Partitioning-the-model" class="headerlink" title="5. Partitioning the model"></a>5. Partitioning the model</h4><pre><code class="python">&quot;&quot;&quot;-------------------------------------------------Partitioning the model-------------------------------------------------&quot;&quot;&quot;mod = partition_for_vitis_ai(mod, params, dpu=vitis_target)</code></pre><h4 id="6-Build-the-partitioned-TVM-module"><a href="#6-Build-the-partitioned-TVM-module" class="headerlink" title="6. Build the partitioned TVM module"></a>6. Build the partitioned TVM module</h4><pre><code class="python">&quot;&quot;&quot;-------------------------------------------------Build the partitioned TVM module-------------------------------------------------&quot;&quot;&quot;export_rt_mod_file = os.path.join(os.getcwd(), &#39;vitis_ai.rtmod&#39;)build_options = &#123;&#39;dpu&#39;: vitis_target,&#39;export_runtime_module&#39;: export_rt_mod_file&#125;with tvm.transform.PassContext(opt_level=3, config=&#123;&#39;relay.ext.vitis_ai.options&#39;: build_options&#125;):lib = relay.build(mod, tvm_target, params=params)</code></pre><h4 id="7-量化模型"><a href="#7-量化模型" class="headerlink" title="7. 量化模型"></a>7. 量化模型</h4><pre><code class="python">&quot;&quot;&quot;-------------------------------------------------Quantize the model-------------------------------------------------&quot;&quot;&quot;QUANT_DIR = os.path.join(&quot;/opt/tvm-vai&quot;, &quot;CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/&quot;)def inputs_func(img_files):inputs = []for img_path in img_files:    image = cv2.imread(img_path)    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)    image = preprocessing(image)    inputs.append(transform_image(image))return inputsprint(&quot;Create InferenceSession for OTF Quantization&quot;)module = graph_executor.GraphModule(lib[&quot;default&quot;](tvm.cpu()))# px_quant_size = int(os.environ[&#39;PX_QUANT_SIZE&#39;]) \#     if &#39;PX_QUANT_SIZE&#39; in os.environ else 128px_quant_size = 128print(&quot;Start OTF Quantization on first &#123;&#125; images&quot;.format(px_quant_size))quant_files = [os.path.join(QUANT_DIR, f) for f in os.listdir(QUANT_DIR)         if f.endswith((&#39;JPEG&#39;, &#39;jpg&#39;, &#39;png&#39;))][:px_quant_size]quant_images = inputs_func(quant_files)print(&#39;Loaded &#123;&#125; inputs successfully.&#39;.format(len(quant_images)))for i in range(px_quant_size):module.set_input(input_name, quant_images[i])module.run()</code></pre><h4 id="8-导出模型"><a href="#8-导出模型" class="headerlink" title="8. 导出模型"></a>8. 导出模型</h4><pre><code class="python">&quot;&quot;&quot;-------------------------------------------------Export and run on a Zynq edge device-------------------------------------------------&quot;&quot;&quot;if vitis_target.startswith(&#39;DPUCZDX8G&#39;):# Export runtime moduletemp = utils.tempdir()lib.export_library(temp.relpath(&quot;tvm_lib.so&quot;))# Build and export lib for aarch64 targettvm_target = tvm.target.arm_cpu(&#39;ultra96&#39;)lib_kwargs = &#123;    &#39;fcompile&#39;: contrib.cc.create_shared,    &#39;cc&#39;: &quot;/usr/aarch64-linux-gnu/bin/ld&quot;&#125;build_options = &#123;    &#39;load_runtime_module&#39;: export_rt_mod_file&#125;with tvm.transform.PassContext(opt_level=3, config=&#123;&#39;relay.ext.vitis_ai.options&#39;: build_options&#125;):    lib_dpuczdx8g = relay.build(mod, tvm_target, params=params)lib_dpuczdx8g.export_library(&#39;tvm_dpu_cpu.so&#39;, **lib_kwargs)else:lib.export_library(&#39;tvm_dpu_cpu.so&#39;)print(&quot;Finished storing the compiled model as tvm_dpu_cpu.so&quot;)print(&quot;Finished OTF Quantization&quot;)</code></pre><h3 id="三、推理"><a href="#三、推理" class="headerlink" title="三、推理"></a>三、推理</h3><p>&amp;ensp;&amp;ensp;推理过程主要包括一下步骤：</p><pre><code>注意框架参数选择TF/ONNX（默认TF）图像预处理 =&gt;加载模型 =&gt; model.run后处理（包括bounding box的nms以及图片写回等）</code></pre><p>&amp;ensp;&amp;ensp;下图是在zcu102上的测试运行结果<br><img src="https://s1.ax1x.com/2022/03/20/qefL7t.png" alt="图片描述">  </p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录一下TVM编译yolov3_tf模型的例子，FPGA开发板：zcu102  &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="TVM" scheme="https://naughtyrabbit.github.io/categories/TVM/"/>
    
    
    <category term="TVM" scheme="https://naughtyrabbit.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>TVM编译模型（tf的例子解读）</title>
    <link href="https://naughtyrabbit.github.io/2022/03/14/TVM-CperModExa-tf/"/>
    <id>https://naughtyrabbit.github.io/2022/03/14/TVM-CperModExa-tf/</id>
    <published>2022-03-14T14:15:19.000Z</published>
    <updated>2022-03-21T04:16:30.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录一下TVM编译一个使用tf框架模型的例子，FPGA开发板：zcu102  </p></blockquote><span id="more"></span><h3 id="一、流程记录"><a href="#一、流程记录" class="headerlink" title="一、流程记录"></a>一、流程记录</h3><p>&amp;ensp;&amp;ensp;需要注意的是，显然（我就不知道） <a href="https://tvm.apache.org/docs/how_to/deploy/vitis_ai.html">https://tvm.apache.org/docs/how_to/deploy/vitis_ai.html</a>   TVM官方这里给的流程，不是完整脚本，只是一个介绍，完整Py脚本可以在编译模型的相关网页最下面下载得到。<br><img src="https://s1.ax1x.com/2022/03/14/bXxDqU.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;过程跟着 <a href="https://github.com/Xilinx/Vitis-AI/blob/1.4/external/tvm/docs/compiling_a_model.md">https://github.com/Xilinx/Vitis-AI/blob/1.4/external/tvm/docs/compiling_a_model.md</a>  这个走。  </p><ol><li>运行容器镜像：.&#x2F;docker_run.sh tvm.ci_vai_1x  </li><li>进入tf虚拟环境  </li><li>进入example文件夹</li><li>编译生成对应目标的文件 python3 compile_mxnet_resnet_18.py “DPUCZDX8G-zcu102”<br>最后一个参数指<strong>DPU target ID</strong> 参照： <a href="https://github.com/Xilinx/Vitis-AI/blob/1.4/external/tvm/README.md#dpu-targets">https://github.com/Xilinx/Vitis-AI/blob/1.4/external/tvm/README.md#dpu-targets</a><br>然后就生成一个如图的 .so 文件，在FPGA上调用。<br><img src="https://s1.ax1x.com/2022/03/14/bXvMBF.png" alt="图片描述"></li></ol><h3 id="二、编译脚本解析"><a href="#二、编译脚本解析" class="headerlink" title="二、编译脚本解析"></a>二、编译脚本解析</h3><p>&amp;ensp;&amp;ensp;显然，自己的网络部署不能用同样的脚本，因此对官方的编译脚本作分析，以实现客制化部署自己的网络。下面是具体的分析（代码段部分是完整的compile_mxnet_resnet_18.py，注释部分是自加的），也是本文重点。  </p><h4 id="1-imports"><a href="#1-imports" class="headerlink" title="1. imports"></a>1. imports</h4><p>&amp;ensp;&amp;ensp;需要注意的是，脚本中导入<strong>pyxir</strong>等模块并未直接使用（在该脚本代码中体现），但是相关模块的导入以及<strong>DPU</strong>目标的声明仍是不可或缺的，否则在执行时会出错。  </p><pre><code class="python">import osimport sysimport numpy as npimport cv2import timefrom typing import Listfrom pathlib import Path# pyxir是tvm和vitis-ai集成的接口，但是这个脚本中似乎只是引入，没有使用该模块？# 然而教程中特意强调了必须引入该模块# pycharm中显示为灰色，建议尝试注释后是否能够正确生成.so文件# 不能，必须加，注释掉会其实缺少文件import pyxir# 这句同理也是灰色import pyxir.contrib.target.DPUCADF8Himport loggingimport tvmfrom tvm import contribimport tvm.relay as relayfrom tvm.relay import transformfrom tvm.contrib import utils, graph_executor as graph_runtime# 这句也是灰色from tvm.contrib.target import vitis_aifrom tvm.relay.build_module import bind_params_by_namefrom tvm.relay.op.contrib.vitis_ai import partition_for_vitis_ai</code></pre><h4 id="2-添加路径"><a href="#2-添加路径" class="headerlink" title="2. 添加路径"></a>2. 添加路径</h4><p>&amp;ensp;&amp;ensp;在安装TVM的过程中，会把tvm-vai的路径写入环境变量（容器内），在此处获取到。相关文件比如&#x2F;opt&#x2F;tvm-vai相关文件都是容器内的，不在宿主环境。</p><pre><code class="python">FILE_DIR   = os.path.dirname(os.path.abspath(__file__))# 使用 os.getenv() 函数获取环境变量TVM_VAI_HOME   = os.getenv(&#39;TVM_VAI_HOME&#39;)QUANT_DIR = os.path.join(TVM_VAI_HOME, &#39;CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/&#39;)if not os.path.exists(QUANT_DIR):    raise ValueError(&quot;Could not find directory &quot;                     &quot;~/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/.&quot;                     &quot; Please install using following commands before&quot;                     &quot; running this example: \n&quot;                     &quot; $ python3 -m ck pull repo:ck-env\n&quot;                     &quot; $ python3 -m ck install package:imagenet-2012-val-min\n&quot;                     &quot; $ cp -r $HOME/CK-TOOLS $TVM_VAI_HOME&quot;)</code></pre><p><img src="https://s1.ax1x.com/2022/03/16/qSMzK1.png" alt="图片描述">   </p><h4 id="3-下载模型"><a href="#3-下载模型" class="headerlink" title="3. 下载模型"></a>3. 下载模型</h4><pre><code class="python">####################################################################### Download Resnet18 model from Gluon Model Zoo# ---------------------------------------------# In this section, we download a pretrained imagenet model and classify an image.###############################################################################from tvm.contrib.download import download_testdatafrom mxnet.gluon.model_zoo.vision import get_modelfrom PIL import Image#from matplotlib import pyplot as plt# 在线下载模型，存储至block# mxnet下api可参考：https://mxnet.apache.org/versions/1.7.0/api/python/docs/api/gluon/model_zoo/index.html#mxnet.gluon.model_zoo.vision.get_modelblock = get_model(&#39;resnet18_v1&#39;, pretrained=True)# 这个网址是一张猫的图片，不科学上网可能会获取超时，可以改成下面的# img_url = &#39;https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true&#39;img_url = &#39;https://s1.ax1x.com/2022/03/15/bvB7Ct.png&#39;img_name = &#39;cat.png&#39;synset_url = &#39;&#39;.join([&#39;https://gist.githubusercontent.com/zhreshold/&#39;,                      &#39;4d0b62f3d01426887599d4f7ede23ee5/raw/&#39;,                      &#39;596b27d23537e5a1b5751d2b0481ef172f58b539/&#39;,                      &#39;imagenet1000_clsid_to_human.txt&#39;])synset_name = &#39;imagenet1000_clsid_to_human.txt&#39;# 注意这个download_testdata函数默认是不重写的，会一直使用最初的图片img_path = download_testdata(img_url, &#39;cat.png&#39;, module=&#39;data&#39;)synset_path = download_testdata(synset_url, synset_name, module=&#39;data&#39;)with open(synset_path) as f:    synset = eval(f.read())def transform_image(image):    image = np.array(image) - np.array([123., 117., 104.])    image /= np.array([58.395, 57.12, 57.375])    image = image.transpose((2, 0, 1))    image = image[np.newaxis, :]    return image</code></pre><p>&amp;ensp;&amp;ensp;下载下来的模型参数保存至如图位置，赋值给block<br><img src="https://s1.ax1x.com/2022/03/16/qSwTNd.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;下载下来的图片以及txt保存至如图位置<br><img src="https://s1.ax1x.com/2022/03/16/qSBcy6.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;内容如下<br><img src="https://s1.ax1x.com/2022/03/16/qSD23n.png" alt="图片描述">  </p><h4 id="4-模型设置"><a href="#4-模型设置" class="headerlink" title="4. 模型设置"></a>4. 模型设置</h4><pre><code class="python">################################################################################ MODEL SETTINGS## Parameter settings for compiling a model using tvm-vai flow# quant_dir      : path to images for quantization# dpu_target         : hardware accelerator to run the compiled model#                      options: &#39;DPUCADF8H&#39;,  &#39;DPUCZDX8G-zcu104&#39;, &#39;DPUCZDX8G-zcu102&#39;# tvm_target     :# lib_kwargs     : ################################################################################ 检查 python3 compile_mxnet_resnet_18.py &quot;DPUCZDX8G-zcu102&quot; 这句命令是否带参数if len(sys.argv) &lt; 2:    raise ValueError(&quot;No DPU target specified. Please run with &#39;python3 compile_mxnet_resnet_18.py `DPU_TARGET`&#39;&quot;\                     &quot; DPU_TARGET options: &#39;DPUCADF8H&#39;, &#39;DPUCAHX8H-u50lv&#39;, &#39;DPUCAHX8H-u50lv_dwc&#39;, &#39;DPUCAHX8H-u55c_dwc&#39;, &#39;DPUCZDX8G-zcu104&#39;, &#39;DPUCZDX8G-zcu102&#39;&quot;)input_name  = &#39;data&#39;# 原图(cat)shape是256*256# imagenet的图片是96x96# input_shape是模型输入的shape# 测试图片经过预处理之后会转成这个shapeinput_shape = (1,3,224,224)shape_dict  = &#123;input_name:input_shape&#125;dpu_target  = str(sys.argv[1])tvm_target  = &#39;llvm&#39;lib_kwargs  = &#123;&#125;</code></pre><h4 id="5-输入预处理"><a href="#5-输入预处理" class="headerlink" title="5. 输入预处理"></a>5. 输入预处理</h4><pre><code class="python">################################################################################ INPUTS FUNC## Define and inputs function which takes in an iterator value and returns a# dictionary mapping from input name to array containing dataset inputs. Note # that the input function should always return image data in NCHW layout as # all models are converted to NCHW layout internally for Vitis-AI compilation.# # This is necessary for quantizating the model for acceleration using Vitis-AI.###############################################################################def inputs_func(img_files: List[str]):    inputs = []    for img_path in img_files:        img = Image.open(img_path)        img = img.convert(&#39;RGB&#39;)        img = img.resize(input_shape[2:])               inputs.append(transform_image(img))    return inputs</code></pre><h4 id="6-Partition-and-build-the-Model"><a href="#6-Partition-and-build-the-Model" class="headerlink" title="6. Partition and build the Model"></a>6. Partition and build the Model</h4><pre><code class="python">################################################################################ PARTITION &amp; BUILD# # Use TVM Module pass to annotate and partition Relay graph for Vitis-AI acceleration. Targets can be &#39;DPUCADF8H&#39;, &#39;DPUCZDX8G-zcu104&#39;, &#39;DPUCZDX8G-zcu102&#39;# Afterwards build graph using standard TVM flow.############################################################################### 利用中继（relay）api加载模型mod, params = relay.frontend.from_mxnet(block, shape_dict)# 得到（vitis_ai.rtmod）？大概？在python命令行中复现到这一句时报错# invalid pointermod = partition_for_vitis_ai(mod, params, dpu=dpu_target)export_rt_mod_file = os.path.join(os.getcwd(), &#39;vitis_ai.rtmod&#39;)build_options = &#123;    &#39;dpu&#39;: dpu_target,    &#39;export_runtime_module&#39;: export_rt_mod_file&#125;# The partitioned model is passed to the TVM compiler to generate the runtime libraries for the TVM Runtime.# 为目标编译运行时库with tvm.transform.PassContext(opt_level=3, config=&#123;&#39;relay.ext.vitis_ai.options&#39;: build_options&#125;):       lib = relay.build(mod, tvm_target, params=params)</code></pre><h4 id="7-对模型量化"><a href="#7-对模型量化" class="headerlink" title="7. 对模型量化"></a>7. 对模型量化</h4><p>&amp;ensp;&amp;ensp;<del>这一部分，不是很理解，所产生的<strong>quant_images</strong>在这一部分最后一步输入给了InferenceSession,但是对InferenceSession在下一部分的导出中没有起到作用，然后在最后的最后，把这个InferenceSession给删了。但是尝试将这一段以及最后一句del InferenceSession注释之后能产生.so文件（大小和之前不同），但是上板子运行调用报错。</del><br>&amp;ensp;&amp;ensp;这一部分第一句InferenceSession &#x3D; graph_runtime.GraphModule(lib<a href="tvm.cpu()">“default”</a>)，即是加载模型到InferenceSession.然后最后一步InferenceSession.run()，即是对模型进行量化。这里<strong>lib</strong>也是由上一步最后一句由relay.build产生。<br>&amp;ensp;&amp;ensp;下载下面是待量化的图片的位置<br><img src="https://s1.ax1x.com/2022/03/17/q9R6XQ.png" alt="图片描述"><br><img src="https://s1.ax1x.com/2022/03/17/q9RBff.png" alt="图片描述"> </p><pre><code class="python">############################################################## Quantization using first N inputs## ## Usually, to be able to accelerate inference of Neural ## Network models with Vitis-AI DPU accelerators, those models ## need to quantized upfront. In the TVM Vitis AI ## flow we make use of On-The-Fly (OTF) Quantization ## to remove this additional preprocessing step. In this flow,## one doesn&#39;t need to quantize his/her model upfront but can ## make use of the typical inference execution calls ## (InferenceSession.run) to quantize the model on-the-fly ## using the first N inputs. This will set up and calibrate## the Vitis-AI DPU and from that point onwards inference ## will be accelerated for all next inputs.## Set the number of inputs used for quantization to e.g. 8 ## using the PX_QUANT_SIZE environment variable if you want## to quantize on fewer inputs. The default is 128.############################################################print(&quot;Create InferenceSession for OTF Quantization&quot;)# 详见：https://github.com/apache/tvm/blob/main/python/tvm/contrib/graph_executor.py line:114InferenceSession = graph_runtime.GraphModule(lib[&quot;default&quot;](tvm.cpu()))px_quant_size = int(os.environ[&#39;PX_QUANT_SIZE&#39;]) \    if &#39;PX_QUANT_SIZE&#39; in os.environ else 128print(&quot;Start OTF Quantization on first &#123;&#125; images&quot;.format(px_quant_size))# 这里只取了前128张图片，根据上面注释的说法，相当于 first N inputs# 之后的输入也会自动加速quant_files = [os.path.join(QUANT_DIR, f) for f in os.listdir(QUANT_DIR)             if f.endswith((&#39;JPEG&#39;, &#39;jpg&#39;, &#39;png&#39;))][:px_quant_size]#quant_images = inputs_func(quant_files)print(&#39;Loaded &#123;&#125; inputs successfully.&#39;.format(len(quant_images)))for i in range(px_quant_size):    InferenceSession.set_input(input_name, quant_images[i])     # print(&quot;running&quot;)     # 执行量化，过程耗时耗内存    InferenceSession.run()print(&quot;Finished OTF Quantization&quot;)</code></pre><p>&amp;ensp;&amp;ensp;下图是整个<strong>inputs_func</strong>函数的功能（以第一张图片为例），注意此部分代码块不是原脚本中顺序，为了方便理解</p><pre><code class="python"># 获取原始图片quant_images = inputs_func(quant_files) ====&gt;进入inputs_func():def inputs_func(img_files: List[str]):    inputs = []    for img_path in img_files:        # 以第一个图片为例 size(500, 375);title(0, 0, 500, 375);model = &#39;RGB&#39;        img = Image.open(img_path)        img = img.convert(&#39;RGB&#39;)        # &lt;PIL.Image.Image image mode=RGB size=224x224 at 0x1FEF70F0D68&gt;        img = img.resize(input_shape[2:])               inputs.append(transform_image(img))     ====&gt;进入transform_image()):    return inputsdef transform_image(image):    # image : ndarray:(224, 224, 3) [[[ 54.  70.  85.],  [ 56.  72.  87.],  [ 58.  74.  89.],  ...,    # size : 150528 = 224*224*3    image = np.array(image) - np.array([123., 117., 104.])    # image : ndarray:(224, 224, 3) [[[0.92473671 1.2254902  1.48148148],  [0.95898621 1.2605042  1.51633987],  [0.99323572 1.29551821 1.55119826],  ...,    image /= np.array([58.395, 57.12, 57.375])    # image : ndarray:(3, 224, 224) [[[0.92473671 0.95898621 0.99323572 ... 1.13023375 1.09598425 1.06173474],  [0.95898621 0.97611097 0.97611097 ...    image = image.transpose((2, 0, 1))    # image : ndarray:(1, 3, 224, 224) [[[[0.92473671 0.95898621 0.99323572 ... 1.13023375 1.09598425 1.06173474],  [0.95898621 0.97611097 0.97611097 ...    image = image[np.newaxis, :]    return image</code></pre><h4 id="8-导出库"><a href="#8-导出库" class="headerlink" title="8. 导出库"></a>8. 导出库</h4><p>&amp;ensp;&amp;ensp;对于DPUZDX8G目标板需要根据aarch64进行rebuild</p><pre><code class="python">########################################################## Export compiled model for execution ##########################################################if dpu_target.startswith(&#39;DPUCZDX8G&#39;):    # Export runtime module    temp = utils.tempdir()    lib.export_library(temp.relpath(&quot;tvm_lib.so&quot;))    # Build and export lib for aarch64 target    tvm_target = tvm.target.arm_cpu(&#39;ultra96&#39;)    lib_kwargs = &#123;        &#39;fcompile&#39;: contrib.cc.create_shared,        &#39;cc&#39;: &quot;/usr/aarch64-linux-gnu/bin/ld&quot;    &#125;    build_options = &#123;        &#39;load_runtime_module&#39;: export_rt_mod_file    &#125;    with tvm.transform.PassContext(opt_level=3, config=&#123;&#39;relay.ext.vitis_ai.options&#39;: build_options&#125;):        lib_dpuczdx8g = relay.build(mod, tvm_target, params=params)    lib_dpuczdx8g.export_library(&#39;tvm_dpu_cpu.so&#39;, **lib_kwargs)else:    lib.export_library(&#39;tvm_dpu_cpu.so&#39;)print(&quot;Finished storing compiled model as tvm_dpu_cpu.so&quot;)del InferenceSession</code></pre>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录一下TVM编译一个使用tf框架模型的例子，FPGA开发板：zcu102  &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="TVM" scheme="https://naughtyrabbit.github.io/categories/TVM/"/>
    
    
    <category term="TVM" scheme="https://naughtyrabbit.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>TVM环境搭建</title>
    <link href="https://naughtyrabbit.github.io/2022/03/12/TVM-setup/"/>
    <id>https://naughtyrabbit.github.io/2022/03/12/TVM-setup/</id>
    <published>2022-03-12T14:31:17.000Z</published>
    <updated>2022-03-13T05:47:56.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录一下TVM环境搭建过程，FPGA开发板：zcu102  </p></blockquote><span id="more"></span><h3 id="一、主机环境搭建"><a href="#一、主机环境搭建" class="headerlink" title="一、主机环境搭建"></a>一、主机环境搭建</h3><p>&amp;ensp;&amp;ensp;大部分的流程跟着官方文档：<a href="https://tvm.apache.org/docs/how_to/deploy/vitis_ai.html%EF%BC%8C">https://tvm.apache.org/docs/how_to/deploy/vitis_ai.html，</a> 这篇的内容是利用Vitis-AI环境去搭建安装VTM。这里重点记录一些需要注意的地方。</p><h4 id="1-docker相关"><a href="#1-docker相关" class="headerlink" title="1.docker相关"></a>1.docker相关</h4><h5 id="1-1-docker容器拉取"><a href="#1-1-docker容器拉取" class="headerlink" title="1.1 docker容器拉取"></a>1.1 docker容器拉取</h5><p>&amp;ensp;&amp;ensp;利用<strong>docker</strong>去<strong>pull</strong>官方的<strong>container</strong>(容器)的时候可以换源来加速（由于要下载的内容较多，科学上网中途可能因为安全问题被断开连接）。daocloud的加速教程：<a href="http://guide.daocloud.io/dcs/daocloud-9153151.html#docker-toolbox">http://guide.daocloud.io/dcs/daocloud-9153151.html#docker-toolbox</a>    </p><h5 id="1-2-GPG-ERROR"><a href="#1-2-GPG-ERROR" class="headerlink" title="1.2 GPG ERROR"></a>1.2 GPG ERROR</h5><pre><code>GPG error: https://apt.kitware.com/ubuntu bionic InRelease:The following signatures couldn&#39;t be verified because the public kev is not available: NO_PUBKEY 6AF7F99730B3F0A4</code></pre><p>&amp;ensp;&amp;ensp;按照网上教程直接在命令行输入：RUN apt-key adv –keyserver keyserver.ubuntu.com –recv-keys 6AF7F09730B3F0A4  会提示<strong>failed</strong>。应该在<strong>Dockerfile.demo_vitis_ai</strong>文件中作如图修改，另外注意在脚本执行到这一句的时候不要用科学上网，之后获取ubuntu的archrive的时候科学上网会快一点，在此处会直接导致脚本执行失败。<br><img src="https://s1.ax1x.com/2022/03/12/bH5BDO.png" alt="图片描述">  </p><h5 id="1-3-python版本错误"><a href="#1-3-python版本错误" class="headerlink" title="1.3 python版本错误"></a>1.3 python版本错误</h5><pre><code>This script does not work on Python 3.6 The minimum supported Python version is 3.7. Please use https://bootstrap.pypa.io/pip/3.6/get-pip.py instead.The command &#39;/bin/sh -c bash /install/ubuntu_install_python.sh&#39; returned a non-zero code: 1  解决：修改tvm/docker/install/ubuntu_install_python.sh 中https链接为错误提示链接。即：# 修改前（line:36~37）：# Install pipcd /tmp &amp;&amp; wget -q https://bootstrap.pypa.io/get-pip.py &amp;&amp; python3.6 get-pip.py# 修改后（line:36~37）：# Install pipcd /tmp &amp;&amp; wget -q https://bootstrap.pypa.io/pip/3.6/get-pip.py &amp;&amp; python3.6 get-pip.py</code></pre><h4 id="2-TVM-build相关"><a href="#2-TVM-build相关" class="headerlink" title="2.TVM build相关"></a>2.TVM build相关</h4><h4 id="2-1-cmake错误"><a href="#2-1-cmake错误" class="headerlink" title="2.1 cmake错误"></a>2.1 cmake错误</h4><pre><code>CMake Error at /opt/vitis_at/conda/envs/vitis-ai-pytorch/lib/cmake/GTest/GTestTargets. cmake:103 (message): The imported target &quot;GTest:: gmock&quot;references the file      &quot;/opt/vitis_ai/conda/envs/vitis-at-pytorch/lib/libgmock. so&quot;but this file does not extst. Possible reasons include:  解决：根据github上的issue：https://github.com/apache/tvm/issues/9772  修改 tvm/cmake/config.cmake（Line 359）        ORIGINAL:set(USE_GTEST AUTO)      UPDATE: set(USE_GTEST OFF)</code></pre><h4 id="2-2-pytorch环境下，cmake错误"><a href="#2-2-pytorch环境下，cmake错误" class="headerlink" title="2.2 pytorch环境下，cmake错误"></a>2.2 pytorch环境下，cmake错误</h4><pre><code>CMake Error at cmake/modules/contrib/VitisAI.cmake:36 (message):  Can&#39;t build TVM with Vitis-AI because PyXIR can&#39;t be foundCall Stack (most recent call first):  CMakeLists.txt:479 (include)</code></pre><p>&amp;ensp;&amp;ensp;在tensorflow环境下不会报错，两者python版本一致（3.6）。在python命令行中尝试<strong>import pyxir</strong>，同样也是在tensorflow环境中正确导入而在pytorch环境中提示无法找到。暂时没有解决。最终还是在tf环境下完成了TVM的安装。  </p><h3 id="二、target环境搭建"><a href="#二、target环境搭建" class="headerlink" title="二、target环境搭建"></a>二、target环境搭建</h3><p>&amp;ensp;&amp;ensp;一开始没有找到官方文档（或者说不是适配的官方文档，<a href="https://www.xilinx.com/htmldocs/vitis_ai/1_4/installation.html#ariaid-title8">https://www.xilinx.com/htmldocs/vitis_ai/1_4/installation.html#ariaid-title8</a>  这篇Xilinx官网的文档显然不适合本次搭建用的<strong>image</strong>,同时这个链接的教程中提供的<strong>image</strong>就是要用的）。遇到了一些问题：时间不符合（newly created file is older…），wegt的时候certificate不被信任（直接在命令后加上 –no-check-certificate即可）。部分问题可通过走下面的流程避免：<br>&amp;ensp;&amp;ensp;正确的流程是跟着github上的指南：<a href="https://github.com/Xilinx/Vitis-AI/blob/master/external/tvm/docs/running_on_zynq.md">https://github.com/Xilinx/Vitis-AI/blob/master/external/tvm/docs/running_on_zynq.md</a>  其中提供了petalinux setup script。这个脚本给出了完整的安装TVM的runtime的流程。<br>&amp;ensp;&amp;ensp;需要注意的是：  </p><pre><code># Set date/timedate -s &quot;$(wget -qSO- --max-redirect=0 google.com 2&gt;&amp;1 | grep Date: | cut -d&#39; &#39; -f5-8)Z&quot;# 设置时间，如果没有科学上网，把谷歌网址改成百度。# INSTALL PIP3sudo dnf install -y python3-pip  由于image中是不带pip的，脚本中提供了如上命令去安装pip3；但是这个命令会转到Xilinx的某个404网站，即 http://petalinux.xilinx.com/sswreleases/rel-v2021.1/generic/rpm/zynqmpeg/repodata/repomd.xml因此用还是用python3 get-pip.py的方式本地编译生成pip3，这个过程将会持续很长时间</code></pre><p>&amp;ensp;&amp;ensp;此外，由于过程中使用了一些gitee上同步的仓库，部分文件内容不一致，导致最终<strong>cmake</strong>过程中报错：declaration conflict。 需要找到相应第三方包，和官方仓库比对，覆盖。<br>&amp;ensp;&amp;ensp;下面是安装结束后导入tvm以及pyxir均无问题。<br><img src="https://s1.ax1x.com/2022/03/13/bbDwUf.png" alt="图片描述">  </p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录一下TVM环境搭建过程，FPGA开发板：zcu102  &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="TVM" scheme="https://naughtyrabbit.github.io/categories/TVM/"/>
    
    
    <category term="TVM" scheme="https://naughtyrabbit.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>HLS学习入门（FFT（快速傅里叶变换））</title>
    <link href="https://naughtyrabbit.github.io/2022/03/07/hls-learning-FFT/"/>
    <id>https://naughtyrabbit.github.io/2022/03/07/hls-learning-FFT/</id>
    <published>2022-03-07T03:13:07.000Z</published>
    <updated>2022-03-07T08:59:14.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本节介绍了FFT(快速傅里叶变换)的HLS实现，使用tcl脚本创建HLS工程的过程，以及如何在vivado中使用生成的IP。</p></blockquote><span id="more"></span><h3 id="一、FFT原理"><a href="#一、FFT原理" class="headerlink" title="一、FFT原理"></a>一、FFT原理</h3><p>&amp;ensp;&amp;ensp;FFT 是离散傅立叶变换的快速算法，可以将一个信号变换到频域。有些信号在时域上是很难看出什么特征的，<br>但是如果变换到频域之后，就很容易看出特征了。这就是很多信号分析采用 FFT 变换的原因。另外，FFT 可以将一<br>个信号的频谱提取出来，这在频谱分析方面也是经常用的。<br>&amp;ensp;&amp;ensp;FFT 结果的物理意义网上有一大神圈圈对此做了详细的描述，我们在这里摘录如下方便大家理解 FFT。<br>&amp;ensp;&amp;ensp;一个模拟信号，经过 ADC 采样之后，就变成了数字信号。根据采样定理，采样频率要大于信号频率的两倍。采<br>样得到的数字信号，就可以做 FFT 变换了。N 个采样点，经过 FFT 之后，就可以得到 N 个点的 FFT 结果。为了方便<br>进行 FFT 运算，通常 N 取 2 的整数次方。<br>&amp;ensp;&amp;ensp;假设采样频率为 Fs，信号频率 F，采样点数为 N。那么 FFT 之后结果就是一个为 N 点的复数。每一个点就对应<br>着一个频率点。这个点的模值，就是该频率值下的幅度特性。具体跟原始信号的幅度有什么关系呢？假设原始信号<br>的峰值为 A，那么 FFT 的结果的每个点（除了第一个点直流分量之外）的模值就是 A 的 N&#x2F;2 倍。而第一个点就是直<br>流分量，它的模值就是直流分量的 N 倍。而每个点的相位就是在该频率下的信号的相位。第一个点表示直流分量（即<br>0Hz），而最后一个点 N 的再下一个点（实际上这个点是不存在的，这里是假设的第 N+1 个点，也可以看做是将第<br>一个点分做两半分，另一半移到最后）则表示采样频率 Fs，中间被 N-1 个点平均分成 N 等份，每个点的频率依次增<br>加。例如某点 n 所表示的频率为：Fn&#x3D;(n-1)<em>Fs&#x2F;N。由上面的公式可以看出，Fn 所能分辨到频率为为 Fs&#x2F;N，如果采<br>样频率 Fs 为 1024Hz，采样点数为 1024 点，则可以分辨到 1Hz。1024Hz 的采样率采样 1024 点，刚好是 1 秒，也就<br>是说，采样 1 秒时间的信号并做 FFT，则结果可以分析到 1Hz，如果采样 2 秒时间的信号并做 FFT，则结果可以分析<br>到 0.5Hz。如果要提高频率分辨力，则必须增加采样点数，也即采样时间。频率分辨率和采样时间是倒数关系。<br>&amp;ensp;&amp;ensp;假设 FFT 之后某点 n 用复数 a+bi 表示，那么这个复数的模就是 An&#x3D;根号 a</em>a+b<em>b，相位就是 Pn&#x3D;atan2(b,a)。<br>根据以上的结果，就可以计算出 n 点（n≠1，且 n&lt;&#x3D;N&#x2F;2）对应的信号的表达式为：An&#x2F;(N&#x2F;2)<em>cos(2</em>pi</em>Fn<em>t+Pn)， 即 2</em>An&#x2F;N<em>cos(2</em>pi<em>Fn</em>t+Pn)。<br>对于 n&#x3D;1 点的信号，是直流分量，幅度即为 A1&#x2F;N。<br>&amp;ensp;&amp;ensp;由于 FFT 结果的对称性，通常我们只使用前半部分的结果，即小于采样频率一半的结果。<br>&amp;ensp;&amp;ensp;假设我们有一个信号，它含有 2V 的直流分量，频率为 50Hz、相位为-30 度、幅度为 3V 的交流信号，以及一个<br>频 率 为 75Hz 、相位为 90 度 、 幅 度 为 1.5V 的交流信号。用数学表达式就是如下：<br>S&#x3D;2+3<em>cos(2</em>pi<em>50</em>t-pi<em>30&#x2F;180)+1.5</em>cos(2<em>pi</em>75<em>t+pi</em>90&#x2F;180)<br>&amp;ensp;&amp;ensp;式中 cos 参数为弧度，所以-30 度和 90 度要分别换算成弧度。我们以 256Hz 的采样率对这个信号进行采样，总<br>共采样 256 点。按照我们上面的分析，Fn&#x3D;(n-1)*Fs&#x2F;N，我们可以知道，每两个点之间的间距就是 1Hz，第 n 个点的频率就是 n-1。我们的信号有 3 个频率：0Hz、50Hz、75Hz，应该分别在第 1 个点、第 51 个点、第 76 个点上出现峰<br>值，其它各点应该接近 0。实际情况如何呢？<br>&amp;ensp;&amp;ensp;我们来看看 FFT 的结果的模值如图所示。<br><img src="https://s1.ax1x.com/2022/03/07/bsb1XV.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;从图中我们可以看到，在第 1 点、第 51 点、和第 76 点附近有比较大的值。我们分别将这三个点附近的数据拿<br>上来细看：  </p><pre><code>1 点： 512+0i2 点： -2.6195E-14 - 1.4162E-13i3 点： -2.8586E-14 - 1.1898E-13i50 点：-6.2076E-13 - 2.1713E-12i51 点：332.55 - 192i52 点：-1.6707E-12 - 1.5241E-12i75 点：-2.2199E-13 -1.0076E-12i76 点：3.4315E-12 + 192i77 点：-3.0263E-14 +7.5609E-13i</code></pre><p>&amp;ensp;&amp;ensp;很明显，1 点、51 点、76 点的值都比较大，它附近的点值都很小，可以认为是 0，即在那些频率点上的信号幅<br>度为 0。接着，我们来计算各点的幅度值。分别计算这三个点的模值，<br>结果如下：  </p><pre><code>1 点： 51251 点：38476 点：192</code></pre><p>&amp;ensp;&amp;ensp;按照公式，可以计算出直流分量为：512&#x2F;N&#x3D;512&#x2F;256&#x3D;2；50Hz 信号的幅度为：384&#x2F;(N&#x2F;2)&#x3D;384&#x2F;(256&#x2F;2)&#x3D;3；75Hz<br>信号的幅度为 192&#x2F;(N&#x2F;2)&#x3D;192&#x2F;(256&#x2F;2)&#x3D;1.5。可见，从频谱分析出来的幅度是正确的。<br>&amp;ensp;&amp;ensp;然后再来计算相位信息。直流信号没有相位可言，不用管它。先计算 50Hz 信号的相位，atan2(-192,<br>332.55)&#x3D;-0.5236,结果是弧度，换算为角度就是 180*(-0.5236)&#x2F;pi&#x3D;-30.0001。再计算 75Hz 信号的相位，atan2(192,<br>3.4315E-12)&#x3D;1.5708 弧度，换算成角度就是 180*1.5708&#x2F;pi&#x3D;90.0002。可见，相位也是对的。根据 FFT 结果以及上<br>面的分析计算，我们就可以写出信号的表达式了，它就是我们开始提供的信号。<br>&amp;ensp;&amp;ensp;总结：假设采样频率为 Fs，采样点数为 N，做 FFT 之后，某一点 n（n 从 1 开始）表示的频率为：Fn&#x3D;(n-1)*Fs&#x2F;N；<br>该点的模值除以 N&#x2F;2 就是对应该频率下的信号的幅度（对于直流信号是除以 N）；该点的相位即是对应该频率下的<br>信号的相位。相位的计算可用函数 atan2(b,a)计算。atan2(b,a)是求坐标为(a,b)点的角度值，范围从-pi 到 pi。<br>要精确到 xHz，则需要采样长度为 1&#x2F;x 秒的信号，并做 FFT。要提高频率分辨率，就需要增加采样点数，这在一些<br>实际的应用中是不现实的，需要在较短的时间内完成分析。解决这个问题的方法有频率细分法，比较简单的方法是<br>采样比较短时间的信号，然后在后面补充一定数量的 0，使其长度达到需要的点数，再做 FFT，这在一定程度上能<br>够提高频率分辨力。具体的频率细分法可参考相关文献。  </p><h3 id="二、使用tcl脚本创建工程"><a href="#二、使用tcl脚本创建工程" class="headerlink" title="二、使用tcl脚本创建工程"></a>二、使用tcl脚本创建工程</h3><p>&amp;ensp;&amp;ensp;在官方的参考手册 ug871-vivado-high-level-synthesis-tutorial.pdf 中对 FFT 有比较详细的介绍，并且官<br>方提供的 Example 中也有对应的工程方便学习<br>&amp;ensp;&amp;ensp;脚本内容：</p><pre><code class="tcl">puts &#123;==            Hello Boards                     ==&#125;############################################### Project settings# HLS工程参数set hls_prj_name    fft_ifft_hls          ;#指定工程路径名set prj_name        fft_ifft              ;#指定工程名set part            xc7z020clg400-2       ;#指定芯片类型set Period          10                    ;#指定时钟周期set Img_name        test_1080p.bmp        ;#指定视频图片文件名set image           0                     ;#是否添加图片文件进行仿真1--添加 0--不添加set run_cosim       0                     ;#是否进行C_RTL联合仿真1--仿真 0--不仿真set run_frontend_prj 1set run_backend_prj  1set run_csim         1set run_cosim        1# 获取当前路径.../tclset path  [pwd]# 返回上层目录...set dir   [file dirname $path]# 判断是否存在prj文件夹，没有则新建一个set prj   [file join [file join $dir &quot;prj&quot;]]if &#123;![file isdirectory $prj ]&#125; &#123;    file mkdir $prj&#125;# 判断是否在prj里面存在HLS工程文件夹，没有则新建一个set hls_prj   [file join [file join $prj [file join $hls_prj_name]]]if &#123;![file isdirectory $hls_prj ]&#125; &#123;    file mkdir $hls_prj&#125;puts &#123;==         Miz701N Create a New Project         ==&#125;# Create a project# ****************************************************************************# real2xfft project# ****************************************************************************cd $hls_prjif &#123; $run_frontend_prj &#125; &#123;    open_project -reset real2xfft_prj    # The source file and test bench    set hls_src   [file join [file join $hls_prj &quot;src&quot;]]    puts $hls_src    cd $hls_src        set_top hls_real2xfft    add_files $hls_src/real2xfft.cpp    add_files -tb xfft2real.cpp    add_files -tb hls_realfft_test.cpp    if &#123; $run_csim &#125; &#123;        open_solution &quot;solution1&quot;        set_part $part        create_clock -period $Period        csim_design -clean    &#125;    open_solution &quot;solution1&quot;    set_part $part    create_clock -period $Period    csynth_design    if &#123;$run_cosim&#125; &#123;        cosim_design -trace_level all -rtl verilog    &#125;    export_design&#125;# ****************************************************************************# xfft2real project# ****************************************************************************cd $hls_prjif &#123; $run_backend_prj &#125; &#123;    open_project -reset xfft2real_prj    # The source file and test bench    set hls_src   [file join [file join $hls_prj &quot;src&quot;]]    puts $hls_src    cd $hls_src    set_top hls_xfft2real    add_files $hls_src/xfft2real.cpp    add_files -tb real2xfft.cpp    add_files -tb hls_realfft_test.cpp    open_solution &quot;solution1&quot;    set_part $part    create_clock -period $Period    csynth_design    export_design&#125;exit</code></pre><p>&amp;ensp;&amp;ensp;打开Vivado HLS 2019.2Command Prompt<br>&amp;ensp;&amp;ensp;输入：vivado_hls -f run_hls.tcl<br><img src="https://s1.ax1x.com/2022/03/07/bytacT.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;结束之后会生成IP。<br><img src="/url" alt="图片描述">  </p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本节介绍了FFT(快速傅里叶变换)的HLS实现，使用tcl脚本创建HLS工程的过程，以及如何在vivado中使用生成的IP。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="hls" scheme="https://naughtyrabbit.github.io/categories/hls/"/>
    
    
    <category term="hls学习" scheme="https://naughtyrabbit.github.io/tags/hls%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>HLS学习入门（索贝尔算子1）</title>
    <link href="https://naughtyrabbit.github.io/2022/03/06/hls-learning-sobel/"/>
    <id>https://naughtyrabbit.github.io/2022/03/06/hls-learning-sobel/</id>
    <published>2022-03-06T03:35:02.000Z</published>
    <updated>2022-03-06T07:47:06.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本节介绍了一种使用 HLS 实现 Sobel 检测的方法</p></blockquote><span id="more"></span><h3 id="一、Sobel-原理介绍"><a href="#一、Sobel-原理介绍" class="headerlink" title="一、Sobel 原理介绍"></a>一、Sobel 原理介绍</h3><p>&amp;ensp;&amp;ensp;索贝尔算子（Sobel operator）主要用作边缘检测，在技术上，它是一离散性差分算子，用来运算图像亮度函<br>数的灰度之近似值。在图像的任何一点使用此算子，将会产生对应的灰度矢量或是其法矢量。<br>&amp;ensp;&amp;ensp;Sobel 卷积因子为：<br><img src="https://s1.ax1x.com/2022/03/06/bB52Ux.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;该算子包含两组 3x3 的矩阵，分别为横向及纵向，将之与图像作平面卷积，即可分别得出横向及纵向的亮度差<br>分近似值。如果以 A 代表原始图像，Gx 及 Gy 分别代表经横向及纵向边缘检测的图像灰度值，其公式如下：<br><img src="https://s1.ax1x.com/2022/03/06/bB55xe.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;具体计算如下：  </p><pre><code>Gx = (-1)*f(x-1, y-1) + 0*f(x,y-1) + 1*f(x+1,y-1)        +(-2)*f(x-1,y) + 0*f(x,y)+2*f(x+1,y)        +(-1)*f(x-1,y+1) + 0*f(x,y+1) + 1*f(x+1,y+1)    = [f(x+1,y-1)+2*f(x+1,y)+f(x+1,y+1)]-[f(x-1,y-1)+2*f(x-1,y)+f(x-1,y+1)]Gy =1* f(x-1, y-1) + 2*f(x,y-1)+ 1*f(x+1,y-1)        +0*f(x-1,y) 0*f(x,y) + 0*f(x+1,y)        +(-1)*f(x-1,y+1) + (-2)*f(x,y+1) + (-1)*f(x+1, y+1)    = [f(x-1,y-1) + 2f(x,y-1) + f(x+1,y-1)]-[f(x-1, y+1) + 2*f(x,y+1)+f(x+1,y+1)]</code></pre><p>&amp;ensp;&amp;ensp;其中 f(a,b), 表示图像(a,b)点的灰度值；<br>&amp;ensp;&amp;ensp;图像的每一个像素的横向及纵向灰度值通过以下公式结合，来计算该点灰度的大小<br><img src="https://s1.ax1x.com/2022/03/06/bBIiaq.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;通常，为了提高效率 使用不开平方的近似值：<br><img src="https://s1.ax1x.com/2022/03/06/bBI3i6.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;如果梯度 G 大于某一阀值 则认为该点(x,y)为边缘点。<br>然后可用以下公式计算梯度方向：<br><img src="https://s1.ax1x.com/2022/03/06/bBIULd.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;Sobel 算子根据像素点上下、左右邻点灰度加权差，在边缘处达到极值这一现象检测边缘。对噪声具有平滑作<br>用，提供较为精确的边缘方向信息，边缘定位精度不够高。当对精度要求不是很高时，是一种较为常用的边缘检测<br>方法。</p><h3 id="二、代码解析"><a href="#二、代码解析" class="headerlink" title="二、代码解析"></a>二、代码解析</h3><pre><code class="c++">#include &quot;top.h&quot;void hls_sobel(AXI_STREAM&amp; INPUT_STREAM, AXI_STREAM&amp; OUTPUT_STREAM, int rows, int cols)&#123;    //Create AXI streaming interfaces for the core    #pragma HLS INTERFACE axis port=INPUT_STREAM    #pragma HLS INTERFACE axis port=OUTPUT_STREAM    #pragma HLS RESOURCE core=AXI_SLAVE variable=rows metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cols metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=return metadata=&quot;-bus_bundle CONTROL_BUS&quot;    //指定这两个参数在函数执行过程中不会被改变    #pragma HLS INTERFACE ap_stable port=rows    #pragma HLS INTERFACE ap_stable port=cols    RGB_IMAGE img_0(rows, cols);    RGB_IMAGE img_1(rows, cols);    RGB_IMAGE img_2(rows, cols);    RGB_IMAGE img_3(rows, cols);    RGB_IMAGE img_4(rows, cols);    RGB_IMAGE img_5(rows, cols);    RGB_PIXEL pix(10, 10, 10);    #pragma HLS dataflow    hls::AXIvideo2Mat(INPUT_STREAM, img_0);    //Sobel:Computes a horizontal or vertical Sobel filter, returning    //an estimate of the horizontal or vertical derivative, using a filter    //第三个参数代表size，支持3,5,7,；    //前两个参数是(1, 0)=水平导数；(0, 1)=&gt;垂直导数    hls::Sobel&lt;1,0,3&gt;(img_0, img_1);    //SubS Computes the differences between elements of image src and scalar value scl    hls::SubS(img_1, pix, img_2);    /*     * 经过scale处理     */       //Scale Converts an input image src with optional linear transformation       //后面两个参数是变换比例和偏移，简单的说就是y=kx+b的k,b。       hls::Scale(img_2, img_3, 2, 0);       hls::Erode(img_3, img_4);       hls::Dilate(img_4, img_5);       hls::Mat2AXIvideo(img_5, OUTPUT_STREAM);/* //未经scale处理    hls::Erode(img_2, img_3);    hls::Dilate(img_3, img_4);    hls::Mat2AXIvideo(img_4, OUTPUT_STREAM); */&#125;</code></pre><h3 id="三、仿真结果"><a href="#三、仿真结果" class="headerlink" title="三、仿真结果"></a>三、仿真结果</h3><p>&amp;ensp;&amp;ensp;下图是提取边缘信息的仿真结果，分别是HLS实现和opencv实现：之后export RTL导出IP以供vivado使用。<br><img src="https://s1.ax1x.com/2022/03/06/bDPAFf.png" alt="图片描述">  </p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本节介绍了一种使用 HLS 实现 Sobel 检测的方法&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="hls" scheme="https://naughtyrabbit.github.io/categories/hls/"/>
    
    
    <category term="hls学习" scheme="https://naughtyrabbit.github.io/tags/hls%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>HLS学习入门（霍夫圆检测）</title>
    <link href="https://naughtyrabbit.github.io/2022/03/05/hls-learning-hough/"/>
    <id>https://naughtyrabbit.github.io/2022/03/05/hls-learning-hough/</id>
    <published>2022-03-05T01:29:37.000Z</published>
    <updated>2022-03-05T06:05:38.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>学习使用 HLS 设计一个基于 Hough 变换的圆检测算法</p></blockquote><span id="more"></span><h3 id="一、Hough变换原理"><a href="#一、Hough变换原理" class="headerlink" title="一、Hough变换原理"></a>一、Hough变换原理</h3><p>&amp;ensp;&amp;ensp;霍夫变换(Hough Transform)是图像处理中的一种特征提取技术，该过程在一个参数空间中通过计算累计结果<br>的局部最大值得到一个符合该特定形状的集合作为霍夫变换结果。霍夫变换于 1962 年由 Paul Hough 首次提出，后<br>于 1972 年由 Richard Duda 和 Peter Hart 推广使用，经典霍夫变换用来检测图像中的直线，后来霍夫变换扩展到<br>任意形状物体的识别，多为圆和椭圆。霍夫变换运用两个坐标空间之间的变换将在一个空间中具有相同形状的曲线<br>或直线映射到另一个坐标空间的一个点上形成峰值，从而把检测任意形状的问题转化为统计峰值问题。  </p><h4 id="1-Hough-变换直线检测"><a href="#1-Hough-变换直线检测" class="headerlink" title="1.Hough 变换直线检测"></a>1.Hough 变换直线检测</h4><p>&amp;ensp;&amp;ensp;简单的说，就是：<strong>图像空间中的直线与参数空间中的点是一一对应的，参数空间中的直线与图像空间中的点也是一一对应的</strong><br>说实话我不是很懂百度或者原文中的原理解释，我的理解就是图像空间中的直线由两个参数（k,b）决定，这两个参数在参数空间反映为1点。反之同理<br><img src="https://s4.ax1x.com/2022/03/05/bwVtPK.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;简单的说，比如如图的黑色直线，假定它的参数是[k, b]，现在扫描整幅图像上的点，而经过每一个点都有无数条直线（红色），根据前文我们知道每一条图像空间中的直线都对于参数空间的一个点。因为这条直线上的所有点它所产生的直线簇一定有一条经过原直线（即在参数空间会存在一个点[k, b]与之对应），所以，这一趟扫描下来，会在参数空间这得到较为多的对应点[k ,b]，据此，我们可以认为原图空间存在这样一条直线与参数空间[k ,b]点相对应。</p><h4 id="2-Hough-变换圆检测"><a href="#2-Hough-变换圆检测" class="headerlink" title="2.Hough 变换圆检测"></a>2.Hough 变换圆检测</h4><p>&amp;ensp;&amp;ensp;继使用 hough 变换检测出直线之后，顺着坐标变换的思路，提出了一种检测圆的方法。<br>1 如何表示一个圆？<br>与使用（r,theta）来表示一条直线相似，使用（a,b,r）来确定一个圆心为（a,b）半径为 r 的圆。<br>2 如何表示过某个点的所有圆？<br>某个圆过点（x1,y1），则有：(x1-a1)^2 + (y1-b1)^2 &#x3D; r1^2 。<br>那么过点（x1,y1）的所有圆可以表示为（a1(i),b1(i),r1(i)），其中 r1∈（0，无穷），每一个 i 值都对应一个<br>不同的圆，（a1(i),b1(i),r1(i)）表示了无穷多个过点（x1,y1）的圆。<br>3 如何确定多个点在同一个圆上？<br>如(2)中说明，过点（x1,y1）的所有圆可以表示为（a1(i),b1(i),r1(i)），过点（x2,y2）的所有圆可以表示为<br>（a2(i),b2(i),r2(i)），过点（x3,y3）的所有圆可以表示为（a3(i),b3(i),r3(i)），如果这三个点在同一个圆<br>上 ， 那 么 存 在 一 个 值 （ a0,b0,r0 ） ， 使 得 a0 &#x3D; a1(k)&#x3D;a2(k)&#x3D;a3(k) 且 b0 &#x3D; b1(k)&#x3D;b2(k)&#x3D;b3(k) 且 r0<br>&#x3D; r1(k)&#x3D;r2(k)&#x3D;r3(k)，即这三个点同时在圆（a0,b0,r0)上。<br>从下图可以形象的看出：<br><img src="https://s4.ax1x.com/2022/03/05/bdrRds.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;首先，分析过点（x1,y1）的所有圆（a1(i),b1(i),r1(i)），当确定 r1(i)时 ，（a1(i),b1(i)）的轨迹是一<br>个以（x1,y1,r1(i)）为中心半径为 r1(i)的圆。那么，所有圆（a1(i),b1(i),r1(i)）的组成了一个以（x1,y1,0）<br>为顶点，锥角为 90 度的圆锥面。<br>三个圆锥面的交点 A 既是同时过这三个点的圆。</p><h4 id="3-Hough-变换圆检测流程"><a href="#3-Hough-变换圆检测流程" class="headerlink" title="3.Hough 变换圆检测流程"></a>3.Hough 变换圆检测流程</h4><p>&amp;ensp;&amp;ensp;Hough 变换时一种利用图像的全局特征将特定形状边缘链接起来。它通过点线的对偶性，将源图像上的点影射<br>到用于累加的参数空间，把原始图像中给定曲线的检测问题转化为寻找参数空间中的峰值问题。由于利用全局特征，<br>所以受噪声和边界间断的影响较小，比较鲁棒。<br>&amp;ensp;&amp;ensp;Hough 变换思想为：在原始图像坐标系下的一个点对应了参数坐标系中的一条直线，同样参数坐标系的一条直<br>线对应了原始坐标系下的一个点，然后，原始坐标系下呈现直线的所有点，它们的斜率和截距是相同的，所以它们<br>在参数坐标系下对应于同一个点。这样在将原始坐标系下的各个点投影到参数坐标系下之后，看参数坐标系下有没<br>有聚集点，这样的聚集点就对应了原始坐标系下的直线。<br>&amp;ensp;&amp;ensp;因此采用 hough 变换主要有以下几个步骤：<br>1）Detect the edge<br>&amp;ensp;&amp;ensp;检测得到图像的边缘<br>2）Create accumulator<br>&amp;ensp;&amp;ensp;采用二维向量描述图像上每一条直线区域，将图像上的直线区域计数器映射到参数空间中的存储单元，p 为直<br>线区域到原点的距离，所以对于对角线长度为 n 的图像，p 的取值范围为（0， n），θ 值得取值范围为（0， 360），<br>定义为二维数组 HoughBuf[n][360]为存储单元。<br>&amp;ensp;&amp;ensp;对所有像素点（x，y）在所有 θ 角的时候，求出ρ.从而累加ρ值出现的次数。高于某个阈值的ρ就是一个直<br>线。这个过程就类似于横坐标是 θ 角，ρ就是到直线的最短距离。横坐标 θ 不断变换，根据直线方程公司，ρ &#x3D;<br>xcosθ + ysinθ 对于所有的不为 0 的像素点，计算出ρ，找到ρ在坐标(θ,ρ)的位置累加 1.<br>3) Detect the peaks, maximal in the accumulator<br>&amp;ensp;&amp;ensp;通过统计特性，假如图像平面上有两条直线，那么最终会出现 2 个峰值，累加得到最高的数组的值为所求直线<br>参数  </p><h3 id="二、Hough圆检测的HLS实现"><a href="#二、Hough圆检测的HLS实现" class="headerlink" title="二、Hough圆检测的HLS实现"></a>二、Hough圆检测的HLS实现</h3><p>&amp;ensp;&amp;ensp;我们看下面一个实际问题：我们要从一副图像中检测出半径以知的圆形来。我们可以取和图像平面一样的参数<br>平面，以图像上每一个前景点为圆心，以已知的半径在参数平面上画圆，并把结果进行累加。最后找出参数平面上<br>的峰值点，这个位置就对应了图像上的圆心。在这个问题里，图像平面上的每一点对应到参数平面上的一个圆。<br>&amp;ensp;&amp;ensp;把上面的问题改一下，假如我们不知道半径的值，而要找出图像上的圆来。这样，一个办法是把参数平面扩大<br>称为三维空间。就是说，参数空间变为 x–y–R 三维，对应圆的圆心和半径。图像平面上的每一点就对应于参数空<br>间中每个半径下的一个圆，这实际上是一个圆锥。最后当然还是找参数空间中的峰值点。不过，这个方法显然需要<br>大量的存储空间，运行速度也会是很大问题。<br>&amp;ensp;&amp;ensp;那么有什么比较好的解决方法么?我们前面假定的图像都是黑白图像(二值图像)，实际上这些二值图像多是彩<br>色或灰度图像通过边缘提取来的。我们前面提到过，图像边缘除了位置信息，还有方向信息也很重要，这里就用上<br>了。根据圆的性质，圆的半径一定在垂直于圆的切线的直线上，也就是说， 在圆上任意一点的法线上。这样，解<br>决上面的问题，我们仍采用 2 维的参数空间，对于图像上的每 一前景点，加上它的方向信息，都可以确定出一条<br>直线，圆的圆心就在这条直线上。这样一来，问题就会简单了许多</p><h3 id="三、代码解析"><a href="#三、代码解析" class="headerlink" title="三、代码解析"></a>三、代码解析</h3><pre><code class="c++">#include &quot;top.h&quot;//#include &quot;opencv_top.h&quot;#include &lt;stdio.h&gt;#include &lt;iostream&gt;using namespace std;void hls::hls_hough_line(GRAY_IMAGE &amp;src,GRAY_IMAGE &amp;dst,int rows,int cols)&#123;    GRAY_PIXEL result;    int row ,col,k;    ///参数空间的参数圆心O（a,b）半径radius    int a = 0,b = 0,radius = 0;    //累加器    int A0 = rows;    int B0 = cols;    //注意HLS不支持变长数组，所以这里直接指定数据长度    const int Size = 1089900;//Size = rows*cols*(120-110);    #ifdef __SYNTHESIS__        int _count[Size];        int *count = &amp;_count[0];    #else        //这一步在HLS中不能被综合，仅仿真使用        int*count =(int *) malloc(Size * sizeof(int));    #endif    //偏移    int  index ;    //为累加器赋值0    for (row = 0;row &lt; Size;row++)    &#123;        count[row] = 0;    &#125;    GRAY_PIXEL src_data;    uchar temp0;    for (row = 0; row&lt; rows;row++)    &#123;#pragma HLS PIPELINE II=1 off        for (col = 0; col&lt; cols;col++)        &#123;            src &gt;&gt; src_data;            uchar temp = src_data.val[0];            //检测黑线            if (temp == 0)            &#123;                //遍历a ,b 为;累加器赋值                for (a = 0;a &lt; A0;a++)                &#123;                    for (b = 0;b &lt; B0;b++)                    &#123;                        radius = (int)(sqrt(pow((double)(row-a),(double)2) + pow((double)(col - b),(double)2)));                        if(radius &gt; 110 &amp;&amp; radius &lt; 120)                        &#123;                            index  = A0 * B0 *(radius-110) + A0*b + a;                            count[index]++;                        &#125;                    &#125;                &#125;            &#125;        &#125;    &#125;    //遍历累加器数组，找出所有的圆    for (a = 0 ; a &lt; A0 ; a++)    &#123;#pragma HLS PIPELINE II=1 off        for (b = 0 ; b &lt; B0; b++)        &#123;            for (radius = 110 ; radius &lt; 120; radius++)            &#123;                index  = A0 * B0 *(radius-110) + A0*b + a;                if (count[index] &gt; 210)                &#123;                    //在image2中绘制该圆                    for(k = 0; k &lt; rows;k++)                    &#123;                        for (col = 0 ; col&lt; cols;col++)                        &#123;                        //x有两个值，根据圆公式(x-a)^2+(y-b)^2=r^2得到                        int temp = (int)(sqrt(pow((double)radius,(double)2)- pow((double)(col-b),(double)2)));                        int x1 = a + temp;                            int x2 = a - temp;                            if ( (k == x1)||(k == x2) )&#123;                                result.val[0] = (uchar)255;                            &#125;                            else&#123;                                result.val[0] = (uchar)0;                            &#125;                            dst &lt;&lt; result;                        &#125;                    &#125;                &#125;            &#125;        &#125;    &#125;&#125;void hls_hough(AXI_STREAM&amp; src_axi, AXI_STREAM&amp; dst_axi, int rows, int cols)&#123;#pragma HLS INTERFACE axis port=src_axi#pragma HLS INTERFACE axis port=dst_axi#pragma HLS RESOURCE core=AXI_SLAVE variable=rows metadata=&quot;-bus_bundle CONTROL_BUS&quot;#pragma HLS RESOURCE core=AXI_SLAVE variable=cols metadata=&quot;-bus_bundle CONTROL_BUS&quot;#pragma HLS RESOURCE core=AXI_SLAVE variable=return metadata=&quot;-bus_bundle CONTROL_BUS&quot;#pragma HLS INTERFACE ap_stable port=rows#pragma HLS INTERFACE ap_stable port=cols    GRAY_IMAGE  img_src(rows, cols);    GRAY_IMAGE  img_dst(rows, cols);    #pragma HLS dataflow    hls::AXIvideo2Mat(src_axi,img_src);    hls::hls_hough_line(img_src,img_dst,rows,cols);    hls::Mat2AXIvideo(img_dst,dst_axi);&#125;</code></pre><h3 id="四、仿真结果"><a href="#四、仿真结果" class="headerlink" title="四、仿真结果"></a>四、仿真结果</h3><p><img src="https://s4.ax1x.com/2022/03/05/bwGCFg.png" alt="图片描述">  </p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;学习使用 HLS 设计一个基于 Hough 变换的圆检测算法&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="hls" scheme="https://naughtyrabbit.github.io/categories/hls/"/>
    
    
    <category term="hls学习" scheme="https://naughtyrabbit.github.io/tags/hls%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>HLS学习入门（肤色检测）</title>
    <link href="https://naughtyrabbit.github.io/2022/03/04/hls-learning-SkinDetection/"/>
    <id>https://naughtyrabbit.github.io/2022/03/04/hls-learning-SkinDetection/</id>
    <published>2022-03-04T01:42:02.000Z</published>
    <updated>2022-03-06T04:38:12.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本节通过设计一个肤色检测的算法对前面几章的内容进行了巩固和验证，通过一组仿真验证了整个算法的有<br>效性。</p></blockquote><span id="more"></span><h3 id="一、肤色检测原理"><a href="#一、肤色检测原理" class="headerlink" title="一、肤色检测原理"></a>一、肤色检测原理</h3><p>&amp;ensp;&amp;ensp;肤色作为人的体表显著特征之一,尽管人的肤色因为人种的不同有差异,呈现出不同的颜色,但是在排除了亮度<br>和视觉环境等对肤色的影响后,皮肤的色调基本一致,这就为利用颜色信息来做肤色分割提供了理论的依据。<br>&amp;ensp;&amp;ensp;在肤色识别中,常用的颜色空间为 YCbCr 颜色空间。在 YCbCr 颜色空间中,Y 代表亮度,Cb 和 Cr 分别代表蓝色分<br>量和红色分量,两者合称为色彩分量。YCbCr 颜色空间具有将色度与亮度分离的特点,在 YCbCr 色彩空间中,肤色的聚<br>类特性比较好,而且是两维独立分布,能够比较好地限制肤色的分布区域,并且受人种的影响不大。对比 RGB 颜色空<br>间和 YCbCr 颜色空间,当光强发生变化时,RGB 颜色空间中(R,G,B)会同时发生变化,而 YCbCr 颜色空间中受光强相对<br>独立,色彩分量受光强度影响不大,因此 YCbCr 颜色空间更适合用于肤色识别。<br>&amp;ensp;&amp;ensp;由于肤色在 YCbCr 空间受亮度信息的影响较小，本算法直接考虑 YCbCr 空间的 CbCr 分量，映射为两维独立分<br>布的 CbCr 空间。在 CbCr 空间下，肤色类聚性好，利用人工阈值法将肤色与非肤色区域分开，形成二值图像。<br>RGB 转 YCbCr 的公式为：  </p><pre><code>Y = 0.257*R+0.564*G+0.098*B+16 Cb= -0.148*R-0.291*G+0.439*B+128 Cr = 0.439*R-0.368*G-0.071*B+128  对肤色进行判定的条件常使用如下判定条件:Cb &gt; 77 &amp;&amp; Cb &lt; 127 Cr &gt; 133 &amp;&amp; Cr &lt; 173</code></pre><h3 id="二、代码分析"><a href="#二、代码分析" class="headerlink" title="二、代码分析"></a>二、代码分析</h3><h4 id="1-void-hls-hls-skin-dection-函数"><a href="#1-void-hls-hls-skin-dection-函数" class="headerlink" title="1. void hls::hls_skin_dection()函数"></a>1. void hls::hls_skin_dection()函数</h4><p>&amp;ensp;&amp;ensp;对于不了解的数据类型，可以和其他IDE一样<strong>ctrl</strong>单击找到定义的地方，比如这里<strong>RGB_IMAGE</strong>可以在<strong>top.h</strong>里找到是<strong>mat</strong>类型的重定义。<br>&amp;ensp;&amp;ensp;关于<strong>LOOp</strong>(循环),在HLS中也是支持被综合的，但是有一些限制，比如可变循环边界是不允许的，更多可以参考官方ug902文档。</p><pre><code class="c++">// void hls::hls_skin_dection(原图像，输出图像，图像范围、颜色，亮度分量极小极大值，//蓝色分量，红色分量极小极大值)void hls::hls_skin_dection(RGB_IMAGE&amp; src, RGB_IMAGE&amp; dst,int rows, int cols,        int y_lower,int y_upper,int cb_lower,int cb_upper,int cr_lower,int cr_upper)&#123;    //整体是由两层循环组成，按rows,cols遍历图片    LOOp_ROWS:for(int row = 0; row &lt; rows ; row++)    &#123;       #pragma HLS PIPELINE II=1 off        LOOp_COLS:for(int col = 0; col &lt; cols; col++)        &#123;        //变量定义            RGB_PIXEL src_data;        RGB_PIXEL pix;        RGB_PIXEL dst_data;        //皮肤标志位        bool skin_region;        //如果在图片范围内，就把RGB_IMAGE格式图片信息赋值给RGB_PIXEL格式图片信息            if(row &lt; rows &amp;&amp; col &lt; cols) &#123;            src &gt;&gt; src_data;            &#125;            //获取RGB通道数据            uchar B = src_data.val[0];           uchar G = src_data.val[1];           uchar R = src_data.val[2];           //RGB--&gt;YCbCr颜色空间转换           //先扩大256倍，再右移8位变回。将小数的计算转换成整数的            uchar y  = (76 * R + 150 * G + 29 * B) &gt;&gt; 8;            uchar cb = ((128*B -43*R - 85*G)&gt;&gt;8) + 128 ;            uchar cr = ((128*R -107*G - 21 * B)&gt;&gt;8)+ 128 ;            //肤色区域判定            if (y &gt; y_lower &amp;&amp; y &lt; y_upper &amp;&amp; cb &gt; cb_lower &amp;&amp; cb &lt; cb_upper &amp;&amp; cr &gt; cr_lower &amp;&amp; cr &lt; cr_upper)            skin_region = 1;            else                skin_region = 0;            //在原图上作标记，如果是肤色区域用255（白色）进行替代            uchar temp0= (skin_region == 1)? (uchar)255: B;            uchar temp1= (skin_region == 1)? (uchar)255: G;            uchar temp2= (skin_region == 1)? (uchar)255: R;            dst_data.val[0] = temp0;//输出数据 B            dst_data.val[1] = temp1;//输出数据 G            dst_data.val[2] = temp2;//输出数据 R            dst &lt;&lt; dst_data;        &#125;    &#125;&#125;</code></pre><h4 id="2-顶层函数ImgProcess-Top"><a href="#2-顶层函数ImgProcess-Top" class="headerlink" title="2. 顶层函数ImgProcess_Top()"></a>2. 顶层函数ImgProcess_Top()</h4><p>&amp;ensp;&amp;ensp;没有什么特殊内容，定义输入输出图像，然后将输入图像转成矩阵类型，供上面的函数使用，然后输出图像。</p><pre><code class="c++">void ImgProcess_Top(AXI_STREAM&amp; input, AXI_STREAM&amp; output,int rows, int cols,                    int y_lower,int y_upper,int cb_lower,int cb_upper,int cr_lower,int cr_upper)&#123;//端口约束    #pragma HLS RESOURCE variable=input core=AXIS metadata=&quot;-bus_bundle INPUT_STREAM&quot;    #pragma HLS RESOURCE variable=output core=AXIS metadata=&quot;-bus_bundle OUTPUT_STREAM&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=rows metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cols metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=y_lower metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=y_upper metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cb_lower metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cb_upper metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cr_lower metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cr_upper metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=return metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS INTERFACE ap_stable port=rows    #pragma HLS INTERFACE ap_stable port=cols    #pragma HLS INTERFACE ap_stable port=y_lower    #pragma HLS INTERFACE ap_stable port=y_upper    #pragma HLS INTERFACE ap_stable port=cb_lower    #pragma HLS INTERFACE ap_stable port=cb_upper    #pragma HLS INTERFACE ap_stable port=cr_lower    #pragma HLS INTERFACE ap_stable port=cr_upper    //定义输入输出图像    RGB_IMAGE img_0(rows, cols);    RGB_IMAGE img_1(rows, cols);    #pragma HLS dataflow    //输入数据转换成矩阵类型    hls::AXIvideo2Mat(input,img_0);    //调用上面定义函数    hls::hls_skin_dection(img_0,img_1,rows,cols,y_lower,y_upper,cb_lower,cb_upper,cr_lower,cr_upper);    hls::Mat2AXIvideo(img_1, output);&#125;</code></pre><h3 id="三、优化"><a href="#三、优化" class="headerlink" title="三、优化"></a>三、优化</h3><h4 id="1-循环的优化"><a href="#1-循环的优化" class="headerlink" title="1.循环的优化"></a>1.循环的优化</h4><p>&amp;ensp;&amp;ensp;在ug902中也可以看到<strong>LOOp</strong>循环是可综合和优化的，优化的手段有多种，这里采用了流水线的优化：  </p><pre><code class="c++">#pragma HLS PIPELINE II=1 off  PIPELINE:Reduces the initiation interval by allowing the overlapped execution of operations within loop or function.通过允许并发来优化</code></pre><p><img src="https://s4.ax1x.com/2022/03/04/bUYpkQ.png" alt="图片描述">  </p><h4 id="2-数据流的优化"><a href="#2-数据流的优化" class="headerlink" title="2.数据流的优化"></a>2.数据流的优化</h4><p>&amp;ensp;&amp;ensp;根据官方手册 how_to_accelerate_opencv_applications_using_vivado_hls.pdf 中的描述：<br><img src="https://s4.ax1x.com/2022/03/04/bUNJFe.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;前两个优化在综合的时候给出<strong>warning</strong>，在2019.2版本已经启用，去掉即可。</p><pre><code class="c++">    #pragma HLS RESOURCE variable=input core=AXIS metadata=&quot;-bus_bundle INPUT_STREAM&quot;    #pragma HLS RESOURCE variable=output core=AXIS metadata=&quot;-bus_bundle OUTPUT_STREAM&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=rows metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cols metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=y_lower metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=y_upper metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cb_lower metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cb_upper metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cr_lower metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=cr_upper metadata=&quot;-bus_bundle CONTROL_BUS&quot;    #pragma HLS RESOURCE core=AXI_SLAVE variable=return metadata=&quot;-bus_bundle CONTROL_BUS&quot;</code></pre><h4 id="3-端口约束的优化"><a href="#3-端口约束的优化" class="headerlink" title="3.端口约束的优化"></a>3.端口约束的优化</h4><pre><code class="c++">    #pragma HLS INTERFACE ap_stable port=rows    #pragma HLS INTERFACE ap_stable port=cols    #pragma HLS INTERFACE ap_stable port=y_lower    #pragma HLS INTERFACE ap_stable port=y_upper    #pragma HLS INTERFACE ap_stable port=cb_lower    #pragma HLS INTERFACE ap_stable port=cb_upper    #pragma HLS INTERFACE ap_stable port=cr_lower    #pragma HLS INTERFACE ap_stable port=cr_upperap_stable: No protocol. The interface is a data port. Vivado HLS assumes the data port isalways stable after reset, which allows internal optimizations to remove unnecessary registers</code></pre><h3 id="四、综合以及仿真结果"><a href="#四、综合以及仿真结果" class="headerlink" title="四、综合以及仿真结果"></a>四、综合以及仿真结果</h3><p>&amp;ensp;&amp;ensp;从综合结果中可以看到资源的利用情况，触发器，查找表等。<br><img src="https://s4.ax1x.com/2022/03/04/bUcYYq.png" alt="图片描述">  </p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本节通过设计一个肤色检测的算法对前面几章的内容进行了巩固和验证，通过一组仿真验证了整个算法的有&lt;br&gt;效性。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="hls" scheme="https://naughtyrabbit.github.io/categories/hls/"/>
    
    
    <category term="hls学习" scheme="https://naughtyrabbit.github.io/tags/hls%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>HLS学习入门（图像加载）</title>
    <link href="https://naughtyrabbit.github.io/2022/03/03/hls-learning-imageload/"/>
    <id>https://naughtyrabbit.github.io/2022/03/03/hls-learning-imageload/</id>
    <published>2022-03-03T04:39:46.000Z</published>
    <updated>2022-03-06T08:55:52.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>学习如何加载图片，视频；一些常用的API。</p></blockquote><span id="more"></span><h3 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h3><p>&amp;ensp;&amp;ensp;创建工程的步骤见上一节，这里注意一下，执行C仿真之前，不需要添加源文件，只需要添加test文件。另外注意不要<strong>opencv_top.cpp</strong>文件，否则仿真失败。</p><h3 id="二、图片数据的获取"><a href="#二、图片数据的获取" class="headerlink" title="二、图片数据的获取"></a>二、图片数据的获取</h3><h4 id="1-通过-cvLoadImage-函数加载图片"><a href="#1-通过-cvLoadImage-函数加载图片" class="headerlink" title="1. 通过 cvLoadImage 函数加载图片"></a>1. 通过 cvLoadImage 函数加载图片</h4><p>&amp;ensp;&amp;ensp;<strong>cvSaveImage</strong>函数  </p><blockquote><p>格式：<br>通过 cvLoadImage 函数加载图片格式如下<br>IplImage* src &#x3D; cvLoadImage(INPUT_IMAGE);<br>cvShowImage(“src”,src);    </p></blockquote><p>&amp;ensp;&amp;ensp;函数cvSaveImage保存图像到指定文件。 图像格式的的选择依赖于filename的扩展名，请参考 cvLoadImage。只有8位单通道或者3通道（通道顺序为’BGR’ ）可以使用这个函数保存。如果格式，深度或者通道不符合要求，请先用cvCvtScale 和 cvCvtColor转换；或者使用通用的cvSave保存图像为XML或者YAML格式。  </p><pre><code class="c">方法1 cvLoadImage函数加载图片    IplImage* src = cvLoadImage(INPUT_IMAGE);    //创建头并分配数据    IplImage* dst = cvCreateImage(cvGetSize(src), src-&gt;depth, src-&gt;nChannels);//获取原始图像大小    AXI_STREAM  src_axi, dst_axi;    //将opencv中的IplImage格式的图像数据类型转换成AXI4-Stream格式的图像数据流    //从而可以利用FPGA进行数据处理    IplImage2AXIvideo(src, src_axi);    //image_filter(src_axi, dst_axi, src-&gt;height, src-&gt;width);    AXIvideo2IplImage(src_axi, dst);    cvSaveImage(OUTPUT_IMAGE, dst);    //窗口名，图片    cvShowImage( &quot;result_1080p&quot;,dst);    cvReleaseImage(&amp;src);        //cvWaitKey()函数的功能是使程序暂停，等待用户触发一个按键操作。    //但如果该函数参数设为一个正数，则程序将暂停一段时间，    //时间长为该整数值个毫秒单位，然后继续执行程序，即使用户没有按下任何键。    cvWaitKey();</code></pre><h4 id="2-通过-imread-函数加载图片"><a href="#2-通过-imread-函数加载图片" class="headerlink" title="2. 通过 imread 函数加载图片"></a>2. 通过 imread 函数加载图片</h4><p>&amp;ensp;&amp;ensp;<strong>imread</strong>函数  </p><blockquote><p>格式：<br>通过imread函数读取图片，格式如下<br>Mat src_rgb &#x3D; imread(INPUT_IMAGE);<br>IplImage src &#x3D; src_rgb;<br>cvShowImage(“src”,&amp;src);  </p></blockquote><pre><code class="c">//方法2 imread函数加载图片    Mat src_rgb = imread(INPUT_IMAGE,CV_LOAD_IMAGE_GRAYSCALE);//加载图片并灰度显示    IplImage src = src_rgb;    cvSaveImage(OUTPUT_IMAGE, &amp;src);    cvShowImage(&quot;src&quot;,&amp;src);    waitKey(0);    return 0;</code></pre><h3 id="三、视频数据的获取"><a href="#三、视频数据的获取" class="headerlink" title="三、视频数据的获取"></a>三、视频数据的获取</h3><p>&amp;ensp;&amp;ensp;1. <strong>cvCaptureFromAVI</strong>函数  </p><blockquote><p>格式：cvCaptureFromAVI(“AVI 文件名称”);<br>功能：函数进行视频文件的载入，用来播放 AVI 文件视频；  </p></blockquote><p>&amp;ensp;&amp;ensp;<strong>cvCaptureFromAVI</strong>跟<strong>cvCaptureFromFile</strong>,<strong>cvCreateFileCapture</strong>都是一样的作用；文件的类型<br>不一定必须是 AVI 格式，只要文件符合 OpenCV 支持的格式就能播放。<br>&amp;ensp;&amp;ensp;2. <strong>cvGrabFrame</strong>函数  </p><blockquote><p>格式：int cvGrabFrame(CvCapture 结构体);<br>功能：将 capture 抓下來的相片放在 OpenCV 中；其与 cvQueryFrame()是相同的步骤；<br>cvGrabFrame()返回值为 0 或 1；0 是失败,1 是成功  </p></blockquote><p>&amp;ensp;&amp;ensp;3. <strong>cvGrabFrame</strong>函数  </p><blockquote><p>格式：cvRetrieveFrame(CvCapture 结构);<br>功能：从 OpenCV 快取中得到 Frame，并配置给 IplImage 结构体；其中：<br>cvQueryFrame()&#x3D;cvGrabFrame()+cvRetrieveFrame(). </p></blockquote><pre><code class="c">//读取视频文件    IplImage *frame;    CvCapture *capture = cvCaptureFromAVI(&quot;1.avi&quot;);//获取视频数据    cvNamedWindow(&quot;AVI player&quot;,0);    while(true)    &#123;        if(cvGrabFrame(capture))        &#123;            frame = cvRetrieveFrame(capture);            cvShowImage(&quot;AVI player&quot;,frame);            if(cvWaitKey(10)&gt;=0) break;        &#125;        else        &#123;            break;        &#125;    &#125;    cvReleaseCapture(&amp;capture);    cvDestroyWindow(&quot;AVI player&quot;);    return 0;</code></pre><h3 id="四、摄像头数据的获取"><a href="#四、摄像头数据的获取" class="headerlink" title="四、摄像头数据的获取"></a>四、摄像头数据的获取</h3><p>&amp;ensp;&amp;ensp;1. <strong>cvCaptureFromCAM</strong>函数  </p><blockquote><p>格式：CvCapture*cvCaptureFromCAM( int index );<br>参数：index，要使用的摄像头索引。    </p></blockquote><p>&amp;ensp;&amp;ensp;2. <strong>cvReleaseCapture</strong>函数  </p><blockquote><p>功能：释放（cvCaptureFromCAM）这个结构，使用函数 cvReleaseCapture。  </p></blockquote><p>&amp;ensp;&amp;ensp;3. <strong>cvWriteFrame</strong>函数  </p><blockquote><p>功能：要将视频写入文件中，使用 cvWriteFrame 写入一帧到一个视频文件中<br>格式：int cvWriteFrame( CvVideoWriter* writer, const IplImage* image );</p></blockquote><pre><code class="c">    //摄像头操作    IplImage *frame;    CvCapture *capture = cvCaptureFromCAM(0);//捕获摄像头数据0--笔记本自带摄像头 1--外部摄像头    cvNamedWindow(&quot;AVI player&quot;,0);    while(true)    &#123;        if(cvGrabFrame(capture))        &#123;            frame = cvRetrieveFrame(capture);            cvShowImage(&quot;AVI player&quot;,frame);            if(cvWaitKey(10)&gt;=0) break;        &#125;        else        &#123;            break;//没有采集到视频数据退出        &#125;    &#125;    cvReleaseCapture(&amp;capture);    cvDestroyWindow(&quot;AVI player&quot;);    return 0;</code></pre>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;学习如何加载图片，视频；一些常用的API。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="hls" scheme="https://naughtyrabbit.github.io/categories/hls/"/>
    
    
    <category term="hls学习" scheme="https://naughtyrabbit.github.io/tags/hls%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>HLS学习入门（流水灯）</title>
    <link href="https://naughtyrabbit.github.io/2022/03/02/hls-learning-shiftled/"/>
    <id>https://naughtyrabbit.github.io/2022/03/02/hls-learning-shiftled/</id>
    <published>2022-03-02T03:29:58.000Z</published>
    <updated>2022-03-02T13:40:52.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>通过流水灯示例来说明HLS工程创建、综合、优化、仿真的流程，以及使用hls工具生成的IP核。</p></blockquote><span id="more"></span><h3 id="一、工作流程"><a href="#一、工作流程" class="headerlink" title="一、工作流程"></a>一、工作流程</h3><h4 id="1-创建工程"><a href="#1-创建工程" class="headerlink" title="1.创建工程"></a>1.创建工程</h4><p>&amp;ensp;&amp;ensp;(1)选择路径，添加source、test文件（暂不添加）<br><img src="https://s4.ax1x.com/2022/03/02/b35pY6.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;(2)选择芯片，这里选择xc78020c1g484-1xc78020c1g484-1（随便选的）<br><img src="https://s4.ax1x.com/2022/03/02/b35W9K.png" alt="图片描述"> </p><h4 id="2-添加source以及test文件"><a href="#2-添加source以及test文件" class="headerlink" title="2.添加source以及test文件"></a>2.添加source以及test文件</h4><p><img src="https://s4.ax1x.com/2022/03/02/b3jZJH.png" alt="图片描述">  </p><h4 id="3-添加顶层函数"><a href="#3-添加顶层函数" class="headerlink" title="3.添加顶层函数"></a>3.添加顶层函数</h4><p><img src="https://s4.ax1x.com/2022/03/02/b3vBjI.png" alt="图片描述">  </p><h4 id="4-综合"><a href="#4-综合" class="headerlink" title="4.综合"></a>4.综合</h4><p><img src="https://s4.ax1x.com/2022/03/02/b3xQIS.png" alt="图片描述">  </p><h4 id="5-查看分析报告"><a href="#5-查看分析报告" class="headerlink" title="5.查看分析报告"></a>5.查看分析报告</h4><p><img src="https://s4.ax1x.com/2022/03/02/b3zkwV.png" alt="图片描述">  </p><h4 id="6-优化"><a href="#6-优化" class="headerlink" title="6.优化"></a>6.优化</h4><p>&amp;ensp;&amp;ensp;点击<strong>directive</strong>选择加入优化，可以看到几个优化选项，在官方文档：<strong>ug902-vivado-high-level-synthesis</strong>中可以查看相应的说明：<br><img src="https://s4.ax1x.com/2022/03/02/b8PYM4.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;<strong>destination</strong>选择<strong>source file</strong>则会对当前目录所有<strong>solution</strong>都作同样优化；选<strong>directive file</strong>只对当前<strong>solution</strong>作优化:<br><img src="https://s4.ax1x.com/2022/03/02/b8kYQS.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;可以看到程序中已经生成了一条优化的指令：<strong>ap_ovld</strong>意义同样可以在官方文档中找到，这里不放贴图了。</p><blockquote><p>#pragma HLS INTERFACE ap_ovld port&#x3D;led_o   </p></blockquote><h4 id="7-再次综合"><a href="#7-再次综合" class="headerlink" title="7.再次综合"></a>7.再次综合</h4><p>&amp;ensp;&amp;ensp;可以在综合结果中查看资源的使用情况来看优化的效果：<br><img src="https://s4.ax1x.com/2022/03/02/b8EHVH.png" alt="图片描述">  </p><h4 id="8-仿真"><a href="#8-仿真" class="headerlink" title="8.仿真"></a>8.仿真</h4><p>&amp;ensp;&amp;ensp;执行c仿真：<br><img src="https://s4.ax1x.com/2022/03/02/b8eXGR.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;仿真结果：<br><img src="https://s4.ax1x.com/2022/03/02/b8m8Wn.png" alt="图片描述">  </p><h4 id="9-modelsim协同仿真"><a href="#9-modelsim协同仿真" class="headerlink" title="9.modelsim协同仿真"></a>9.modelsim协同仿真</h4><p>&amp;ensp;&amp;ensp;执行协同仿真：<br><img src="https://s4.ax1x.com/2022/03/02/b8Ktyt.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;耗时非常久。。。。。<br>&amp;ensp;&amp;ensp;仿真结果，用modelsim打开，将信号<strong>add wave</strong>：<br><img src="https://s4.ax1x.com/2022/03/02/b8YJSO.png" alt="图片描述">  </p><h4 id="10-导出IP"><a href="#10-导出IP" class="headerlink" title="10.导出IP"></a>10.导出IP</h4><p><img src="https://s4.ax1x.com/2022/03/02/b8N95q.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;这里如果报错，可能是因为官方的原因，需要把时间调到2021以前，或者官网有补丁：<a href="https://support.xilinx.com/s/article/76960?language=en_US">https://support.xilinx.com/s/article/76960?language=en_US</a><br>&amp;ensp;&amp;ensp;导出IP位置<br><img src="https://s4.ax1x.com/2022/03/02/b8yan0.png" alt="图片描述">  </p><h4 id="11-在vivado中使用IP"><a href="#11-在vivado中使用IP" class="headerlink" title="11.在vivado中使用IP"></a>11.在vivado中使用IP</h4><p><img src="https://s4.ax1x.com/2022/03/02/bGM3dK.png" alt="图片描述">  </p><h3 id="二、代码分析"><a href="#二、代码分析" class="headerlink" title="二、代码分析"></a>二、代码分析</h3><p>&amp;ensp;&amp;ensp;头文件<strong>shift_led.h</strong></p><pre><code class="c">#ifndef _SHIFT_LED_H_#define _SHIFT_LED_H_//加入设置int自定义位宽的头文件#include &quot;ap_int.h&quot;//设置灯半秒动一次，开发板时钟频率是100M//#define MAX_CNT 1000/2  //仅用于仿真，不然时间较长#define MAX_CNT 100000000/2#define SHIFT_FLAG  MAX_CNT-2//int 类型默认位宽32位，显然在板子上是不合适的,改用ap_int.h库中数据类型//typedef int led_t;//typedef int cnt32_t;//计数器typedef ap_fixed&lt;4,4&gt; led_t;//第一个4代表总位宽，第二个4代表整数部分的位宽是4，则小数部分位宽＝4-4=0typedef ap_fixed&lt;32,32&gt; cnt32_t;void shift_led(led_t *led_o,led_t led_i);#endif</code></pre><p>&amp;ensp;&amp;ensp;在官方文档：<strong>ug902-vivado-high-level-synthesis</strong>中可以查看对数据类型<strong>ap_fixed</strong>的说明：<br><img src="https://s4.ax1x.com/2022/03/02/b8p7QJ.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;c文件<strong>shift_led.cpp</strong>  </p><pre><code class="c">#include &quot;shift_led.h&quot;void shift_led(led_t *led_o,led_t led_i)&#123;#pragma HLS INTERFACE ap_vld port=led_i#pragma HLS INTERFACE ap_ovld port=led_o    led_t tmp_led;    cnt32_t i;//for循环的延时变量    tmp_led = led_i;    for(i = 0;i &lt; MAX_CNT;i++)    &#123;        if(i==SHIFT_FLAG)        &#123;            //假设传入是0xe(1110)            //右移3位：1110 =&gt; 0001            //和0001相与：0001&amp;0001 =&gt; 0001            //左移1位：1110 =&gt; 1100            //和1110相与：1100&amp;1110 =&gt; 1100            //两者相加： 0001+1100 =&gt; 1101            tmp_led = ((tmp_led&gt;&gt;3)&amp;0x1) + ((tmp_led&lt;&lt;1)&amp;0xE);//左移            *led_o = tmp_led;        &#125;    &#125;&#125;</code></pre>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;通过流水灯示例来说明HLS工程创建、综合、优化、仿真的流程，以及使用hls工具生成的IP核。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="hls" scheme="https://naughtyrabbit.github.io/categories/hls/"/>
    
    
    <category term="hls学习" scheme="https://naughtyrabbit.github.io/tags/hls%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>yolov5+csl旋转目标检测代码解析——总结篇</title>
    <link href="https://naughtyrabbit.github.io/2022/02/26/pytorch-learning-adv-yolov5-obb-4/"/>
    <id>https://naughtyrabbit.github.io/2022/02/26/pytorch-learning-adv-yolov5-obb-4/</id>
    <published>2022-02-26T08:59:21.000Z</published>
    <updated>2022-03-06T13:22:24.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录学习yolov5+csl旋转目标检测的原理，对前几篇文章作一个总结，添加一些细节。参考知乎 略略略 <a href="https://zhuanlan.zhihu.com/p/358441134%EF%BC%9B">https://zhuanlan.zhihu.com/p/358441134；</a> yangxue <a href="https://zhuanlan.zhihu.com/p/111493759">https://zhuanlan.zhihu.com/p/111493759</a>  </p></blockquote><span id="more"></span><h3 id="一、训练部分"><a href="#一、训练部分" class="headerlink" title="一、训练部分"></a>一、训练部分</h3><h4 id="1-数据加载"><a href="#1-数据加载" class="headerlink" title="1.数据加载"></a>1.数据加载</h4><p>&amp;ensp;&amp;ensp;加载数据的主要过程都在<strong>create_dataloader</strong>这个方法里。<br><img src="https://s4.ax1x.com/2022/02/24/bikXmn.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;下面是该方法的返回值：</p><pre><code class="python">return loader(dataset,            batch_size=batch_size,            shuffle=shuffle and sampler is None,            num_workers=nw,            sampler=sampler,            pin_memory=True,            collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn), dataset</code></pre><p>&amp;ensp;&amp;ensp;需要注意的是，这里返回的<strong>dataset</strong>是原始数据（与我们的<strong>label</strong>基本一致，<strong>clsid</strong>换一下），但是在下面遍历这里所取得的<strong>train_loader</strong>成员的时候会调用 <strong>LoadImagesAndLabels</strong>这个类的专有函数<strong>getitem__(self, index)<strong>，这里重写了这个方法，使得返回将原先[x1, y1, x2, y2, x3, y3, x4, y4]格式标签转换成了旋转目标检测的长边表示法，同时作了一些数据增强（中途print一下label会发现标签在变成长边表示法之前改变了），另一方面这里的</strong>img</strong>输出是经典的[3 , h, w]，RGB格式。<br>&amp;ensp;&amp;ensp;<del>其他有一个小地方要注意一下，在<strong>poly2rbox</strong>这个方法中，根据四点坐标计算中心点坐标以及长短边角度的函数用的是<strong>cv2.minAreaRect(poly)</strong> ，这个方法原意是求任意个数的点集所围成的最小矩形（计算量还是有的），在转换<strong>poly</strong>表示法到长边表示法的过程中似乎与我所理解的有所不同（见下图），我的理解长边很明显是5.</del>  其实这不是个标准矩形，长边表示法是标准矩形。</p><pre><code class="python">def __getitem__(self, index):      &#39;&#39;&#39;      Augment the [clsid poly] labels and trans label format to rbox.      Returns:      img (tensor): (3, height, width), RGB      labels_out (tensor): (n, [None clsid cx cy l s theta gaussian_θ_labels]) θ∈[-pi/2, pi/2)      img_file (str): img_dir       shapes : None or [(h_raw, w_raw), (hw_ratios, wh_paddings)], for COCO mAP rescaling      &#39;&#39;&#39;</code></pre><p><img src="https://s4.ax1x.com/2022/02/24/bioRv4.png" alt="图片描述">  </p><h4 id="2-推理pred"><a href="#2-推理pred" class="headerlink" title="2.推理pred"></a>2.推理pred</h4><p>&amp;ensp;&amp;ensp;首先<strong>train.py</strong>这里改变图片尺寸（&#x2F;255）作归一化，给tensor处理，同时改为浮点类型，shape不变<br><img src="https://s4.ax1x.com/2022/02/26/bZ11U0.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;这里把图片（图片<strong>shape</strong>[b, 3, height, width], RGB）喂给网络,<br><img src="https://s4.ax1x.com/2022/02/25/bkRZO1.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;然后在<strong>forward</strong>里迭代网络的各个层（卷积、C3等）<br><img src="https://s4.ax1x.com/2022/02/25/bkRs6s.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;经过第一层卷积：</p><pre><code class="python">Conv(  (conv): Conv2d(3, 48, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)  (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)  (act): SiLU(inplace=True))</code></pre><p>&amp;ensp;&amp;ensp;之后x的shape变成如图（2， 48， 512，512）<br><img src="https://s4.ax1x.com/2022/02/26/bZ8emj.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;第二层网络结构：经过第二层网络之后的x的shape:(2, 96, 256, 256)</p><pre><code class="python">Conv(  (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)  (act): SiLU(inplace=True))</code></pre><p><img src="https://s4.ax1x.com/2022/02/26/bZ8rcD.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;依此类推，需要注意的是在第12层<strong>concat</strong>层，将当前层（即上一层输出的x）和m.f参数指定的之前保存的某一层结合作list<br><img src="https://s4.ax1x.com/2022/02/26/bZ8bHs.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;在第16层的时候同样，将当前层和m.f参数指定的之前保存的某一层结合作list，下面还有concat层，不作记录<br>&amp;ensp;&amp;ensp;共25层（整体网络结构见上一篇，这里不再给出），下面是最后一层detect层之前，可以看到，过程中根据self.save参数保存了部分层的输出，并且根据m.f参数，指定当前层的输入是单一的上一层输出还是和之前层组成的list<br><img src="https://s4.ax1x.com/2022/02/26/bZGSvF.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;比如detect层的m.f参数是如下：<br><img src="https://s4.ax1x.com/2022/02/26/bZGmvD.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;则意味着要把y中这几个层的输出联合成list<br><img src="https://s4.ax1x.com/2022/02/26/bZG35t.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;detect层内容：  </p><pre><code class="python">Detect(  (m): ModuleList(    (0): Conv2d(192, 603, kernel_size=(1, 1), stride=(1, 1))    (1): Conv2d(384, 603, kernel_size=(1, 1), stride=(1, 1))    (2): Conv2d(768, 603, kernel_size=(1, 1), stride=(1, 1))  ))</code></pre><p>&amp;ensp;&amp;ensp;分别对三个尺度的tensor作卷积，经过detect层之后三个尺度的输出：<br><img src="https://s4.ax1x.com/2022/02/26/bZJUW6.png" alt="图片描述"> </p><h5 id="2-1-detect层细节处理"><a href="#2-1-detect层细节处理" class="headerlink" title="2.1 detect层细节处理"></a>2.1 detect层细节处理</h5><p>&amp;ensp;&amp;ensp;下面是detect层处理的一些细节：<br>&amp;ensp;&amp;ensp;首先进入forward，按lawyer（3层）操作，经过第一次卷积之后x[0]变成如图：<br><img src="https://s4.ax1x.com/2022/02/26/bZYx8P.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;这里把x[i]进行重组（我的理解相当于一个线性层，但不完全一样）：<br><img src="https://s4.ax1x.com/2022/02/26/bZtA5n.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;201是self.no参数，在detect类中定义（如下图），代表着[16个类别，cx,cy,l,s,theta，180个gaussian_theta_labels]，值得注意的是，这201个参数是相对于一个anchor来说的。这里2是bz，3是每一个grid负责的anchor数量，即一个grid产生3个anchor，然后每一张图片，按3个尺度，分成128×128，64×64，32×32个grid。<br><img src="https://s4.ax1x.com/2022/02/26/bZt0VH.png" alt="图片描述">   </p><h4 id="3-损失计算"><a href="#3-损失计算" class="headerlink" title="3.损失计算"></a>3.损失计算</h4><p>&amp;ensp;&amp;ensp;送给loss时pred（三个尺度的输出，yolov5特点）和target的格式：<br><img src="https://s4.ax1x.com/2022/02/27/beORM9.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;<strong>computer_loss</strong>初始化时的一些设置<br><img src="https://s4.ax1x.com/2022/02/27/bm1DP0.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;下面是调用<strong>computer_loss</strong>的<strong>call</strong>方法一部分,重点是最后的<strong>build_targets</strong>方法</p><pre><code class="python">def __call__(self, p, targets):  # predictions, targets, model      &quot;&quot;&quot;      Args:      p (list[P3_out,...]): torch.Size(b, self.na, h_i, w_i, self.no), self.na means the number of anchors scales      targets (tensor): (n_gt_all_batch, [img_index clsid cx cy l s theta gaussian_θ_labels])      Return：      total_loss * bs (tensor): [1]       torch.cat((lbox, lobj, lcls, ltheta)).detach(): [4]      &quot;&quot;&quot;      device = targets.device      lcls, lbox, lobj = torch.zeros(1, device=device), torch.zeros(1, device=device), torch.zeros(1, device=device)      ltheta = torch.zeros(1, device=device)      # tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets      tcls, tbox, indices, anchors, tgaussian_theta = self.build_targets(p, targets)  # targets</code></pre><h5 id="3-1-build-targets方法"><a href="#3-1-build-targets方法" class="headerlink" title="3.1 build_targets方法"></a>3.1 build_targets方法</h5><pre><code class="python">def build_targets(self, p, targets):      &quot;&quot;&quot;      Args:      p (list[P3_out,...]): torch.Size(b, self.na, h_i, w_i, self.no), self.na means the number of anchors scales      targets (tensor): (n_gt_all_batch, [img_index clsid cx cy l s theta gaussian_θ_labels]) pixel      Return：non-normalized data      tcls (list[P3_out,...]): len=self.na, tensor.size(n_filter2)      tbox (list[P3_out,...]): len=self.na, tensor.size(n_filter2, 4) featuremap pixel      indices (list[P3_out,...]): len=self.na, tensor.size(4, n_filter2) [b, a, gj, gi]      anch (list[P3_out,...]): len=self.na, tensor.size(n_filter2, 2)      tgaussian_theta (list[P3_out,...]): len=self.na, tensor.size(n_filter2, hyp[&#39;cls_theta&#39;])      # ttheta (list[P3_out,...]): len=self.na, tensor.size(n_filter2)      &quot;&quot;&quot;</code></pre><p>&amp;ensp;&amp;ensp;这里所做的是先把targets复制3份，并且在每一分的最后一维加上anchor信息，比如第一份最后数字是0，第二份最后数字是2，这也是targets的shape由[95,187]变成[3,95,188]的过程<br><img src="https://s4.ax1x.com/2022/02/27/bm3Tkn.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;这里提取pred的尺度信息<br><img src="https://s4.ax1x.com/2022/02/27/bm8Z0H.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;把第三维的2：6列作放缩（坐标信息，cx,cy,l,s）<br><img src="https://s4.ax1x.com/2022/02/27/bm83jS.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;match过滤部分，让某一尺度的<strong>anchor</strong>预测对应尺度的<strong>targets</strong>，同时可以避免无限制带来的CIOU计算的梯度爆炸问题。<br><img src="https://s4.ax1x.com/2022/02/27/bm6zY6.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;下面这部分是yolov5的正采样部分，细节可以在之前的博客上看到。总之就是把目标中心点那个<strong>grid</strong>相邻两个<strong>grid</strong>都标记为正样本，有助于收敛。<br><img src="https://s4.ax1x.com/2022/02/27/bmrmMd.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;取出<strong>targets</strong>中预测框中心点以及长短边和180个高斯角度类别信息。需要注意的是，这里只取到了倒数第二列，最后一列是anchor信息（前面提到的加入的0,1,2信息）<br><img src="https://s4.ax1x.com/2022/02/27/bmrcQJ.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;<strong>tbox</strong>中信息是中心点坐标以及长短边，且是偏移量（减去了<strong>grid</strong>左下角坐标这种）<br><img src="https://s4.ax1x.com/2022/02/27/bmseYT.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;至此<strong>build_targets</strong>方法就结束了，已经将<strong>targets</strong>由原始的数据转换成了分开的目标类别，目标框，索引，目标高斯角度类别等。  </p><h5 id="3-2-计算损失"><a href="#3-2-计算损失" class="headerlink" title="3.2 计算损失"></a>3.2 计算损失</h5><p>&amp;ensp;&amp;ensp;<strong>pred</strong>同样在三个尺度上迭代<br><img src="https://s4.ax1x.com/2022/02/27/bmsL3F.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;从<strong>indices</strong>中取出信息，信息包括这765个targets的所属batch，所属anchor，以及在grid中的x,y坐标，然后，把这些信息带入到预测pred中，就能取得这765个anchor的预测信息（长度为201的tensor）。<br><img src="https://s4.ax1x.com/2022/02/27/bmszH1.png" alt="图片描述"><br><img src="https://s4.ax1x.com/2022/02/27/bmyuUP.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;这些gridx,y信息也可以佐证，在128尺度时，这些数据的范围都在128以内，64尺度下的范围也同样在64以内<br><img src="https://s4.ax1x.com/2022/02/27/bmyxxg.png" alt="图片描述"><br><img src="https://s4.ax1x.com/2022/02/27/bm69qs.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;之后就是取出相应列的数据和targets数据作loss处理，没什么需要特别注意的。预测框部分采用的是<strong>CIOU</strong>，其他是<strong>BCEloss</strong>。  </p><h3 id="二、检测部分"><a href="#二、检测部分" class="headerlink" title="二、检测部分"></a>二、检测部分</h3><p>&amp;ensp;&amp;ensp;需要注意的是，预测<strong>detect</strong>部分是相较于训练部分多了一些后处理，最主要的是<strong>NMS</strong>,训练的时候不需要<strong>NMS</strong>,<strong>pred</strong>网络出来的是什么就是什么，不需要改动，是要送去损失函数计算损失作反向传播以便下一次更好的推理。也因此，显然检测部分是不需要反向传播的。下面同样从数据的加载代码部分开始看：</p><h4 id="1-模型和图片加载"><a href="#1-模型和图片加载" class="headerlink" title="1.模型和图片加载"></a>1.模型和图片加载</h4><p>&amp;ensp;&amp;ensp;加载模型和一些参数（权重文件的格式、预定的图片长宽，<strong>stride</strong>）<strong>stride</strong>相当与网络模型的输出的尺度和真实图片的尺度之间的比例，便于之后标记等，<br><img src="https://s1.ax1x.com/2022/03/06/bDuq0S.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;加载test图片，共28张图片<br><img src="https://s1.ax1x.com/2022/03/06/bDMuCj.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;同训练部分的加载图片（训练部分还有标签）过程的专有函数<strong>get_item</strong>一样，LoadStreams类的专有函数next:一般都是在这类专有函数中实现对数据的初步处理，然后送给网络。<br><img src="https://s1.ax1x.com/2022/03/06/bDQlJH.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;最开始读取到的图像格式：分辨率+3通道<br><img src="https://s1.ax1x.com/2022/03/06/bDQhY4.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;之后会进行一个resize，具体是到预定的宽高(1024*1024)，当然，输入不一定长宽等长，取src_w&#x2F;des_w,src_h&#x2F;des_h中较小的比例进行缩小，并且补上一定黑边（yolov5特色）<br>&amp;ensp;&amp;ensp;这里在推理之前遍历dataset（过程中就是在调用上面的next专有函数），并进行归一化操作，以便送入网络。<br><img src="https://s1.ax1x.com/2022/03/06/bDlo4g.png" alt="图片描述">  </p><h4 id="2-网络推理"><a href="#2-网络推理" class="headerlink" title="2.网络推理"></a>2.网络推理</h4><p>&amp;ensp;&amp;ensp;第一张图片推理结果：<br><img src="https://s1.ax1x.com/2022/03/06/bD1d2j.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;下面具体看一下推理过程细节（整体和训练的时候类似，有部分处理不同）<br>&amp;ensp;&amp;ensp;detect层之前，与预测过程同样的地方输出不同的是bs和最后一个图像宽高（输入不是1024*1024,所以不一样很正常）<br><img src="https://s1.ax1x.com/2022/03/06/bD31W4.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;第一个尺度下生成128x92(原来是128)个grid，每个grid有201个预测通道<br><img src="https://s1.ax1x.com/2022/03/06/bD320P.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;如果shape不一致，把self.grid,anchor等改成对应的<br><img src="https://s1.ax1x.com/2022/03/06/bD8ujA.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;改成原图片尺度（这一部分在yolov5原理的时候有讲过，可以看之前的内容）：<br><img src="https://s1.ax1x.com/2022/03/06/bDGYa6.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;把除了最后一维度，其他维度打平，得到第一个尺度下的所有anchor输出<br><img src="https://s1.ax1x.com/2022/03/06/bDGWRg.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;然后以此类推，遍历三个尺度，输出是按第一维把三个尺度的输出相加：<br><img src="https://s1.ax1x.com/2022/03/06/bDGOWF.png" alt="图片描述">  </p><h4 id="3-非极大值抑制"><a href="#3-非极大值抑制" class="headerlink" title="3.非极大值抑制"></a>3.非极大值抑制</h4><p>&amp;ensp;&amp;ensp;具体里面就是一系列处理，先通过预定的置信度筛选一部分，然后通过计算IOU，留下较大的。注意cls*obj的操作是在这里面完成的，最后返回符合条件，经过筛选的anchor的index。<br><img src="https://s1.ax1x.com/2022/03/06/bDJ8Sg.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;经过NMS只剩100多个有效输出<br><img src="https://s1.ax1x.com/2022/03/06/brPur9.png" alt="图片描述">  </p><h4 id="4-add-poly"><a href="#4-add-poly" class="headerlink" title="4.add poly"></a>4.add poly</h4><p>&amp;ensp;&amp;ensp;之后就是一些写检测出来的物体的label写到txt里，并且在图像中作标记的过程</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录学习yolov5+csl旋转目标检测的原理，对前几篇文章作一个总结，添加一些细节。参考知乎 略略略 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/358441134%EF%BC%9B&quot;&gt;https://zhuanlan.zhihu.com/p/358441134；&lt;/a&gt; yangxue &lt;a href=&quot;https://zhuanlan.zhihu.com/p/111493759&quot;&gt;https://zhuanlan.zhihu.com/p/111493759&lt;/a&gt;  &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="pytorch" scheme="https://naughtyrabbit.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch学习" scheme="https://naughtyrabbit.github.io/tags/pytorch%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="https://naughtyrabbit.github.io/tags/yolo/"/>
    
  </entry>
  
  <entry>
    <title>yolov5+csl旋转目标检测代码解析3</title>
    <link href="https://naughtyrabbit.github.io/2022/02/25/pytorch-learning-adv-yolov5-obb-3/"/>
    <id>https://naughtyrabbit.github.io/2022/02/25/pytorch-learning-adv-yolov5-obb-3/</id>
    <published>2022-02-25T01:52:54.000Z</published>
    <updated>2022-02-26T08:51:40.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录学习yolov5+csl旋转目标检测的原理，主要是探究代码中pred的生成过程。参考知乎 略略略 <a href="https://zhuanlan.zhihu.com/p/358441134%EF%BC%9B">https://zhuanlan.zhihu.com/p/358441134；</a> yangxue <a href="https://zhuanlan.zhihu.com/p/111493759">https://zhuanlan.zhihu.com/p/111493759</a>  </p></blockquote><span id="more"></span><h3 id="一、debug记录"><a href="#一、debug记录" class="headerlink" title="一、debug记录"></a>一、debug记录</h3><p>&amp;ensp;&amp;ensp;首先<strong>train.py</strong>这里改变图片尺寸（&#x2F;255）作归一化，给tensor处理，同时改为浮点类型，shape不变<br><img src="https://s4.ax1x.com/2022/02/26/bZ11U0.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;这里把图片（图片<strong>shape</strong>[b, 3, height, width], RGB）喂给网络,<br><img src="https://s4.ax1x.com/2022/02/25/bkRZO1.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;然后在<strong>forward</strong>里迭代网络的各个层（卷积、C3等）<br><img src="https://s4.ax1x.com/2022/02/25/bkRs6s.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;经过第一层卷积：</p><pre><code class="python">Conv(  (conv): Conv2d(3, 48, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)  (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)  (act): SiLU(inplace=True))</code></pre><p>&amp;ensp;&amp;ensp;之后x的shape变成如图（2， 48， 512，512）<br><img src="https://s4.ax1x.com/2022/02/26/bZ8emj.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;第二层网络结构：经过第二层网络之后的x的shape:(2, 96, 256, 256)</p><pre><code class="python">Conv(  (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)  (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)  (act): SiLU(inplace=True))</code></pre><p><img src="https://s4.ax1x.com/2022/02/26/bZ8rcD.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;依此类推，需要注意的是在第12层<strong>concat</strong>层，将当前层（即上一层输出的x）和m.f参数指定的之前保存的某一层结合作list<br><img src="https://s4.ax1x.com/2022/02/26/bZ8bHs.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;在第16层的时候同样，将当前层和m.f参数指定的之前保存的某一层结合作list，下面还有concat层，不作记录<br>&amp;ensp;&amp;ensp;共25层，下面是最后一层detect层之前，可以看到，过程中根据self.save参数保存了部分层的输出，并且根据m.f参数，指定当前层的输入是单一的上一层输出还是和之前层组成的list<br><img src="https://s4.ax1x.com/2022/02/26/bZGSvF.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;比如detect层的m.f参数是如下：<br><img src="https://s4.ax1x.com/2022/02/26/bZGmvD.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;则意味着要把y中这几个层的输出联合成list<br><img src="https://s4.ax1x.com/2022/02/26/bZG35t.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;detect层内容：  </p><pre><code class="python">Detect(  (m): ModuleList(    (0): Conv2d(192, 603, kernel_size=(1, 1), stride=(1, 1))    (1): Conv2d(384, 603, kernel_size=(1, 1), stride=(1, 1))    (2): Conv2d(768, 603, kernel_size=(1, 1), stride=(1, 1))  ))</code></pre><p>&amp;ensp;&amp;ensp;分别对三个尺度的tensor作卷积，经过detect层之后三个尺度的输出：<br><img src="https://s4.ax1x.com/2022/02/26/bZJUW6.png" alt="图片描述"> </p><h4 id="1-detect层细节处理"><a href="#1-detect层细节处理" class="headerlink" title="1.detect层细节处理"></a>1.detect层细节处理</h4><p>&amp;ensp;&amp;ensp;下面是detect层处理的一些细节：<br>&amp;ensp;&amp;ensp;首先进入forward，按lawyer（3层）操作，经过第一次卷积之后x[0]变成如图：<br><img src="https://s4.ax1x.com/2022/02/26/bZYx8P.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;这里把x[i]进行重组（我的理解相当于一个线性层，但不完全一样）：<br><img src="https://s4.ax1x.com/2022/02/26/bZtA5n.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;201是self.no参数，在detect类中定义（如下图），代表着[16个类别，cx,cy,l,s,theta，180个gaussian_theta_labels]，值得注意的是，这201个参数是相对于一个anchor来说的。这里2是bz，3是每一个grid负责的anchor数量，即一个grid产生3个anchor，然后每一张图片，按3个尺度，分成128×128，64×64，32×32个grid。<br><img src="https://s4.ax1x.com/2022/02/26/bZt0VH.png" alt="图片描述"> </p><h3 id="二、网络结构"><a href="#二、网络结构" class="headerlink" title="二、网络结构"></a>二、网络结构</h3><p>&amp;ensp;&amp;ensp;这是<strong>self.model</strong>全部内容，共25层，最后一层<strong>detect</strong>层：  </p><pre><code class="python">Sequential(  (0): Conv(    (conv): Conv2d(3, 48, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2), bias=False)    (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (1): Conv(    (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (2): C3(    (cv1): Conv(      (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv3): Conv(      (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (cv1): Conv(          (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (1): Bottleneck(        (cv1): Conv(          (conv): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(48, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (3): Conv(    (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (4): C3(    (cv1): Conv(      (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv3): Conv(      (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (cv1): Conv(          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (1): Bottleneck(        (cv1): Conv(          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (2): Bottleneck(        (cv1): Conv(          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (3): Bottleneck(        (cv1): Conv(          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (5): Conv(    (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (6): C3(    (cv1): Conv(      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv3): Conv(      (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (1): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (2): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (3): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (4): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (5): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (7): Conv(    (conv): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (8): C3(    (cv1): Conv(      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv3): Conv(      (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (cv1): Conv(          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (1): Bottleneck(        (cv1): Conv(          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (9): SPPF(    (cv1): Conv(      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(1536, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)  )  (10): Conv(    (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)    (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (11): Upsample(scale_factor=2.0, mode=nearest)  (12): Concat()  (13): C3(    (cv1): Conv(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv3): Conv(      (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (1): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (14): Conv(    (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)    (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (15): Upsample(scale_factor=2.0, mode=nearest)  (16): Concat()  (17): C3(    (cv1): Conv(      (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv3): Conv(      (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (cv1): Conv(          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (1): Bottleneck(        (cv1): Conv(          (conv): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(96, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (18): Conv(    (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (19): Concat()  (20): C3(    (cv1): Conv(      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv3): Conv(      (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (1): Bottleneck(        (cv1): Conv(          (conv): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(192, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (21): Conv(    (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)    (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)    (act): SiLU(inplace=True)  )  (22): Concat()  (23): C3(    (cv1): Conv(      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv2): Conv(      (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (cv3): Conv(      (conv): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)      (bn): BatchNorm2d(768, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)      (act): SiLU(inplace=True)    )    (m): Sequential(      (0): Bottleneck(        (cv1): Conv(          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )      (1): Bottleneck(        (cv1): Conv(          (conv): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )        (cv2): Conv(          (conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)          (bn): BatchNorm2d(384, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)          (act): SiLU(inplace=True)        )      )    )  )  (24): Detect(    (m): ModuleList(      (0): Conv2d(192, 603, kernel_size=(1, 1), stride=(1, 1))      (1): Conv2d(384, 603, kernel_size=(1, 1), stride=(1, 1))      (2): Conv2d(768, 603, kernel_size=(1, 1), stride=(1, 1))    )  ))</code></pre>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录学习yolov5+csl旋转目标检测的原理，主要是探究代码中pred的生成过程。参考知乎 略略略 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/358441134%EF%BC%9B&quot;&gt;https://zhuanlan.zhihu.com/p/358441134；&lt;/a&gt; yangxue &lt;a href=&quot;https://zhuanlan.zhihu.com/p/111493759&quot;&gt;https://zhuanlan.zhihu.com/p/111493759&lt;/a&gt;  &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="pytorch" scheme="https://naughtyrabbit.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch学习" scheme="https://naughtyrabbit.github.io/tags/pytorch%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="https://naughtyrabbit.github.io/tags/yolo/"/>
    
  </entry>
  
  <entry>
    <title>yolov5+csl旋转目标检测代码解析2</title>
    <link href="https://naughtyrabbit.github.io/2022/02/23/pytorch-learning-adv-yolov5-obb-2/"/>
    <id>https://naughtyrabbit.github.io/2022/02/23/pytorch-learning-adv-yolov5-obb-2/</id>
    <published>2022-02-23T11:03:11.000Z</published>
    <updated>2022-02-24T12:39:12.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录学习yolov5+csl旋转目标检测的原理，主要是探究代码中如何得到theta&#x2F;gauss_theta的target。参考知乎 略略略 <a href="https://zhuanlan.zhihu.com/p/358441134%EF%BC%9B">https://zhuanlan.zhihu.com/p/358441134；</a> yangxue <a href="https://zhuanlan.zhihu.com/p/111493759">https://zhuanlan.zhihu.com/p/111493759</a></p></blockquote><span id="more"></span><h3 id="一、debug记录"><a href="#一、debug记录" class="headerlink" title="一、debug记录"></a>一、debug记录</h3><p>&amp;ensp;&amp;ensp;经过dataloader之后的数据格式如图，cls已经转化为id。此时仍是原本的表示方法。<br><img src="https://s4.ax1x.com/2022/02/23/bC4ZAU.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;在plot_labels里面会有完整的标签格式转换以及高斯窗的csl方法，但是这个函数并没有返回<br><img src="https://s4.ax1x.com/2022/02/23/bC55zd.png" alt="图片描述"><br><img src="https://s4.ax1x.com/2022/02/23/bC5Jrq.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;即最后一个参数为True时，使用csl，并且采用高斯窗函数，csl部分代码是直接从yangxue大佬那里拿来用的。 </p><pre><code class="python">def poly2rbox(polys, num_cls_thata=180, radius=6.0, use_pi=False, use_gaussian=False):    &quot;&quot;&quot;    Trans poly format to rbox format.    Args:        polys (array): (num_gts, [x1 y1 x2 y2 x3 y3 x4 y4])         num_cls_thata (int): [1], theta class num        radius (float32): [1], window radius for Circular Smooth Label        use_pi (bool): True θ∈[-pi/2, pi/2) ， False θ∈[0, 180)    Returns:        use_gaussian True:            rboxes (array):             csl_labels (array): (num_gts, num_cls_thata)        elif             rboxes (array): (num_gts, [cx cy l s θ])     &quot;&quot;&quot;</code></pre><p>&amp;ensp;&amp;ensp;数据处理（转换标签格式，生成<strong>theta</strong>参数等操作），在<strong>LoadImagesAndLabels</strong>类中重写了<strong>getitem</strong>方法。<br><img src="https://s4.ax1x.com/2022/02/24/bikXmn.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;可以看到，取数据的格式和<strong>getitem</strong>的返回值一致。<br><img src="https://s4.ax1x.com/2022/02/24/biVusH.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;在该方法中，默认启用了<strong>csl</strong>高斯窗，<strong>clsid</strong>不上传，在得到<strong>bbox</strong>之后合并。<br><img src="https://s4.ax1x.com/2022/02/24/biIGpF.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;下面我把<strong>poly2rbox</strong>单独拎出来分析：  </p><pre><code class="python">import numpy as npimport torchpi = 3.141592import cv2def gaussian_label_cpu(label, num_class, u=0, sig=4.0):    &quot;&quot;&quot;    转换成CSL Labels：        用高斯窗口函数根据角度θ的周期性赋予gt labels同样的周期性，使得损失函数在计算边界处时可以做到“差值很大但loss很小”；        并且使得其labels具有环形特征，能够反映各个θ之间的角度距离    Args:        label (float32):[1], theta class        num_theta_class (int): [1], theta class num        u (float32):[1], μ in gaussian function        sig (float32):[1], σ in gaussian function, which is window radius for Circular Smooth Label    Returns:        csl_label (array): [num_theta_class], gaussian function smooth label    &quot;&quot;&quot;    x = np.arange(-num_class/2, num_class/2)    y_sig = np.exp(-(x - u) ** 2 / (2 * sig ** 2))    index = int(num_class/2 - label)    return np.concatenate([y_sig[index:],                            y_sig[:index]], axis=0)def regular_theta(theta, mode=&#39;180&#39;, start=-pi/2):    &quot;&quot;&quot;    limit theta ∈ [-pi/2, pi/2)    &quot;&quot;&quot;    assert mode in [&#39;360&#39;, &#39;180&#39;]    cycle = 2 * pi if mode == &#39;360&#39; else pi    theta = theta - start    theta = theta % cycle    return theta + startdef poly2rbox(polys, num_cls_thata=180, radius=6.0, use_pi=False, use_gaussian=False):    &quot;&quot;&quot;    Trans poly format to rbox format.    Args:        polys (array): (num_gts, [x1 y1 x2 y2 x3 y3 x4 y4])        num_cls_thata (int): [1], theta class num        radius (float32): [1], window radius for Circular Smooth Label        use_pi (bool): True θ∈[-pi/2, pi/2) ， False θ∈[0, 180)    Returns:        use_gaussian True:            rboxes (array):            csl_labels (array): (num_gts, num_cls_thata)        elif            rboxes (array): (num_gts, [cx cy l s θ])    &quot;&quot;&quot;    assert polys.shape[-1] == 8    if use_gaussian:        csl_labels = []    rboxes = []    for poly in polys:        poly = np.float32(poly.reshape(4, 2))        # poly: [[1. 1.], [2. 0.], [5. 4.], [4. 5.]]        # cv2.minAreaRect求出点集下的最小面积矩形        (x, y), (w, h), angle = cv2.minAreaRect(poly) # θ ∈ [0， 90]        angle = -angle # θ ∈ [-90， 0]        theta = angle / 180 * pi # 转为pi制        # trans opencv format to longedge format θ ∈ [-pi/2， pi/2]        if w != max(w, h):            w, h = h, w            theta += pi/2        theta = regular_theta(theta) # limit theta ∈ [-pi/2, pi/2)        angle = (theta * 180 / pi) + 90 # θ ∈ [0， 180)        if not use_pi: # 采用angle弧度制 θ ∈ [0， 180)            rboxes.append([x, y, w, h, angle])        else: # 采用pi制            rboxes.append([x, y, w, h, theta])        if use_gaussian:            csl_label = gaussian_label_cpu(label=angle, num_class=num_cls_thata, u=0, sig=radius)            csl_labels.append(csl_label)    if use_gaussian:        return np.array(rboxes), np.array(csl_labels)    return np.array(rboxes)def main():    ploys = torch.tensor([[1, 1, 2, 0, 5, 4, 4, 5],                          [5, 1, 6, 2, 6, 2.5, 5, 1.5]])    rboxes, csl_labels = poly2rbox(polys=ploys,                                   num_cls_thata=180,                                   radius=6.0,                                   use_pi=True, use_gaussian=True)    print(&#39;rboxes: &#39;, rboxes)    print(&#39;csl_labels: &#39;, csl_labels)if __name__ == &quot;__main__&quot;:    main()</code></pre><p>&amp;ensp;&amp;ensp;这是<strong>debug</strong>记录，比较奇怪的是<strong>x,y,w,h</strong>的计算似乎<strong>cv2</strong>的<strong>api</strong>有点问题，比如我给的例子，角度计算的没问题，但是很明显一个3,4,5的三角形，结果h&#x3D;5.199999；短边1:1:1.414(根号2)，结果算出来是w&#x3D;1.39。虽然差距不大，本身api的调用确实可能比较复杂（可以实现求点集下最小面积矩形）。当然实际情况是标注的label可能不一定是完美的矩形，用当作矩形的方法似乎也有不妥。但是这对loss的求解和反向传播肯定是有影响的。至此<strong>target</strong>部分的数据，csl的角度怎么得到的都已经清楚，将在下一篇博客里面探究。<strong>pred</strong>是如何得到的。另外，这个<strong>demo</strong>中<strong>csl_labels</strong>:  (2, 180)<br><img src="https://s4.ax1x.com/2022/02/24/bioRv4.png" alt="图片描述">  </p><h3 id="一、api记录"><a href="#一、api记录" class="headerlink" title="一、api记录"></a>一、api记录</h3><h4 id="1-tqdm（进度条美观）"><a href="#1-tqdm（进度条美观）" class="headerlink" title="1.tqdm（进度条美观）"></a>1.tqdm（进度条美观）</h4><pre><code class="python">from tqdm import tqdmimport timeiterator = tqdm(iterable=range(100),                # iterable：tdqm数据参数支持的数据类型是可迭代的对象iterable，                # 在Python中默认的可迭代对象有：list、str、tuple、dict、file、range等                desc=&#39;test_tqdm&#39;,                # str类型，作为进度条说明，在进度条左边                total=100,                # 预取的迭代次数                leave=True,                # 循环结束后是否保留进度提示信息，默认保留                ncols=100,                # 进度条长度，150比较适合                mininterval=0.1,                # 进度条最小的更新间隔（秒）                maxinterval=10.0,                # 进度条最大的更新间隔（秒）                unit=&#39;it&#39;,                # 单位，默认it每秒迭代数                bar_format=None,                # bar_format=&#39;&#123;l_bar&#125;&#123;bar:10&#125;&#123;r_bar&#125;&#123;bar:-10b&#125;&#39;,                # 在进度条右边添加字典类型描述信息                position=None,                # 指定偏移，这个功能在多个进度条中有用                postfix=None                # 自定义进度条                )for i in iterator:    time.sleep(0.3)    </code></pre><p>&amp;ensp;&amp;ensp;结果如下：<br><img src="https://s4.ax1x.com/2022/02/24/bFcdUg.png" alt="图片描述">  </p><h4 id="2-python专有函数"><a href="#2-python专有函数" class="headerlink" title="2.python专有函数"></a>2.python专有函数</h4><p>&amp;ensp;&amp;ensp;没有python的基础，所以有些基础知识也是边实践边学（轻喷&#x2F;-_-\），类似**<strong>init</strong><strong>函数等的python的专有函数，是在创建一个类对象之后一定会调用的方法，类似于构造函数。同时也可被重载（项目中的</strong><strong>get_item</strong>**就是重载了专有函数），下面是一个简单的例子。  </p><pre><code class="python">class Cat:    def __init__(self, color):        self.color = color    def eat(self):        print(&quot;--eating food--&quot;)    def printinfo(self):        print(self.color)# 实例化Cat对象mimi = Cat(&quot;white&quot;)# 如果创建实例的时候没有给color，此句会报错mimi.printinfo()mimi.eat()# 重新给类成员赋值mimi.color = &quot;black&quot;mimi.printinfo()</code></pre><p>&amp;ensp;&amp;ensp;输出如下：  </p><pre><code class="python">white--eating food--black</code></pre><p>&amp;ensp;&amp;ensp;与此类似的，两个下划线开头（**__private_method**），声明该方法为私有方法，只能在类的内部调用 ，不能在类的外部调用。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录学习yolov5+csl旋转目标检测的原理，主要是探究代码中如何得到theta&amp;#x2F;gauss_theta的target。参考知乎 略略略 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/358441134%EF%BC%9B&quot;&gt;https://zhuanlan.zhihu.com/p/358441134；&lt;/a&gt; yangxue &lt;a href=&quot;https://zhuanlan.zhihu.com/p/111493759&quot;&gt;https://zhuanlan.zhihu.com/p/111493759&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="pytorch" scheme="https://naughtyrabbit.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch学习" scheme="https://naughtyrabbit.github.io/tags/pytorch%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="https://naughtyrabbit.github.io/tags/yolo/"/>
    
  </entry>
  
  <entry>
    <title>yolov5+csl旋转目标检测代码解析1</title>
    <link href="https://naughtyrabbit.github.io/2022/02/17/pytorch-learning-adv-yolov5-obb/"/>
    <id>https://naughtyrabbit.github.io/2022/02/17/pytorch-learning-adv-yolov5-obb/</id>
    <published>2022-02-17T01:50:17.000Z</published>
    <updated>2022-02-22T05:20:22.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录学习yolov5+csl旋转目标检测的原理，主要是代码中损失函数部分的细节。参考知乎 略略略 <a href="https://zhuanlan.zhihu.com/p/358441134%EF%BC%9B">https://zhuanlan.zhihu.com/p/358441134；</a> yangxue <a href="https://zhuanlan.zhihu.com/p/111493759">https://zhuanlan.zhihu.com/p/111493759</a></p></blockquote><span id="more"></span><h3 id="一、Define-criteria"><a href="#一、Define-criteria" class="headerlink" title="一、Define criteria"></a>一、Define criteria</h3><p>&amp;ensp;&amp;ensp;源码位置：utils&#x2F;loss.py</p><pre><code class="python"># Define criteriaBCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h[&#39;cls_pw&#39;]], device=device))BCEtheta = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h[&#39;theta_pw&#39;]], device=device))BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h[&#39;obj_pw&#39;]], device=device))# BCEtheta是相对于yolov5自加的，csl基于角度分类的部分。</code></pre><p>&amp;ensp;&amp;ensp;nn.BCEWithLogits()本质上和nn.BCELoss()没有区别，只是在BCELoss上对pred加了个logits函数(也就是sigmoid函数)，例子如下：  </p><pre><code class="python">import torchimport torch.nn as nnlabel = torch.Tensor([1, 1, 0])pred = torch.Tensor([3, 2, 1])pred_sig = torch.sigmoid(pred)loss = nn.BCELoss()print(loss(pred_sig, label))loss = nn.BCEWithLogitsLoss()print(loss(pred, label))loss = nn.BCEWithLogitsLoss()print(loss(pred_sig, label))</code></pre><p>&amp;ensp;&amp;ensp;输出结果分别为：</p><pre><code class="python">tensor(0.4963)tensor(0.4963)tensor(0.5990)</code></pre><p>&amp;ensp;&amp;ensp;可以看到，nn.BCEWithLogitsLoss()相当于是在nn.BCELoss()中预测结果pred的基础上先做了个sigmoid，然后继续正常算loss。所以这就涉及到一个比较奇葩的bug，如果网络本身在输出结果的时候已经用sigmoid去处理了，算loss的时候用nn.BCEWithLogitsLoss()…那么就会相当于预测结果算了两次sigmoid，可能会出现各种奇奇怪怪的问题——比如网络收敛不了  </p><h3 id="二、loss计算"><a href="#二、loss计算" class="headerlink" title="二、loss计算"></a>二、loss计算</h3><p><img src="https://s4.ax1x.com/2022/02/19/Hb7Xoq.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;loss部分似乎没有过多的修改，在每个原本obj&#x2F;cls等loss出现的地方加上theta（角度）损失就好，角度损失采用分类损失（损失函数都是BCE）。  </p><h3 id="三、head部分"><a href="#三、head部分" class="headerlink" title="三、head部分"></a>三、head部分</h3><p>&amp;ensp;&amp;ensp;backbone部分特征提取不是重点，添加csl的时候也不需要作修改，下面看一下head部分————detect层（位于models&#x2F;yolo.py）<br><img src="https://s4.ax1x.com/2022/02/19/Hb7Xoq.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;如图，输出部分相较于原来+180，即多了180个角度分类结果。之后类似改动（+180）不再赘述。<br>&amp;ensp;&amp;ensp;但是我对比了一下，整个yolo.py只有另外一处也是**no &#x3D; na * (nc + 185)**这种改动，其余改动均是作者加的注释部分。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录学习yolov5+csl旋转目标检测的原理，主要是代码中损失函数部分的细节。参考知乎 略略略 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/358441134%EF%BC%9B&quot;&gt;https://zhuanlan.zhihu.com/p/358441134；&lt;/a&gt; yangxue &lt;a href=&quot;https://zhuanlan.zhihu.com/p/111493759&quot;&gt;https://zhuanlan.zhihu.com/p/111493759&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="pytorch" scheme="https://naughtyrabbit.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch学习" scheme="https://naughtyrabbit.github.io/tags/pytorch%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="https://naughtyrabbit.github.io/tags/yolo/"/>
    
  </entry>
  
  <entry>
    <title>yolov5目标检测原理探究（2）</title>
    <link href="https://naughtyrabbit.github.io/2022/02/02/pytorch-learning-adv-yolov5-2/"/>
    <id>https://naughtyrabbit.github.io/2022/02/02/pytorch-learning-adv-yolov5-2/</id>
    <published>2022-02-02T13:13:45.000Z</published>
    <updated>2022-02-26T12:43:54.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录学习yolov5的原理，包括一些代码细节以及代码的复现。参考薛定谔的AI。</p></blockquote><span id="more"></span><h3 id="一、正样本采样细节"><a href="#一、正样本采样细节" class="headerlink" title="一、正样本采样细节"></a>一、正样本采样细节</h3><p>&amp;ensp;&amp;ensp;相关源代码位置：utils&#x2F;loss.py&#x2F;class ComputeLoss&#x2F;def build_targets(self, p, targets)<br>&amp;ensp;&amp;ensp;对于如图所示的8个目标边框的中心点：<br><img src="https://s4.ax1x.com/2022/02/02/HEeUTf.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;在代码中的如图位置会对目标边框的grid作正采样：<br><img src="https://s4.ax1x.com/2022/02/02/HEeHn1.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;第一部分经过筛选之后得到如图结果（灰色），j,k是所得的布尔矩阵;l,m是以tensor右上角为原点之后作同样的筛选得到。<br><img src="https://s4.ax1x.com/2022/02/02/HEmQH0.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;将所得点作对应的偏移：<br><img src="https://s4.ax1x.com/2022/02/02/HEmBE6.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;最终所得如图所示；其中深灰色即原ground truth对应边框数据，绿色为补充正样本边框数据。<br><img src="https://s4.ax1x.com/2022/02/02/HEm5Uf.png" alt="图片描述">  </p><h3 id="二、损失计算细节"><a href="#二、损失计算细节" class="headerlink" title="二、损失计算细节"></a>二、损失计算细节</h3><p>&amp;ensp;&amp;ensp;相关源代码位置：utils&#x2F;loss.py&#x2F;class ComputeLoss<br><img src="https://s4.ax1x.com/2022/02/03/HEzL1f.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;如图是GIOU，前两点比较好理解，第三点“偏离趋势度量能力”是左下角图2，如果此时采用传统IOU则两者数值都是0，则无法对训练过程作出很好的指导作用；当然当目标框和预测框处于水平或者垂直位置时，GIOU则退化成了传统的IOU，不具备优势，为了解决这一问题，提出DIOU：<br><img src="https://s4.ax1x.com/2022/02/03/HVS6bQ.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;在最新的6.0版本的yolov5中，改为了最新的CIOU计算<br><img src="https://s4.ax1x.com/2022/02/03/HVS42V.png" alt="图片描述">   </p><h3 id="三、损失计算源码细节"><a href="#三、损失计算源码细节" class="headerlink" title="三、损失计算源码细节"></a>三、损失计算源码细节</h3><p><img src="https://s4.ax1x.com/2022/02/03/HVC2UH.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;这里提一下<strong>focal_loss</strong>的作用（yolov5默认没有使用，可能是作者实验得到没有较大或者没有提升，但是我觉得这个理念还是挺好的）。<strong>focal_loss</strong>适用于正负样本比例失调的情况，比如一张图片里有10个正样本，90个负样本。那如果没有<strong>focal_loss</strong>的话，网络就会倾向于预测出90个负样本，毕竟正负样本权重一样，网络在<strong>tell</strong>出这90个样本不是我想要的会比<strong>tell</strong>出这10个样本是我想要的得到更多“<strong>奖励</strong>”。但是显然，我们想要的还是网络分辨出正样本。因此<strong>focal_loss</strong>所做的就是给与这少数的正样本更多的权重来让网络对正样本的预测倾向更高。<br><img src="https://s4.ax1x.com/2022/02/26/bZq4vn.png" alt="图片描述"><br><img src="https://s4.ax1x.com/2022/02/26/bZOa6A.png" alt="图片描述"><br><img src="https://s4.ax1x.com/2022/02/03/HVK5kt.png" alt="图片描述"><br><img src="https://s4.ax1x.com/2022/02/03/HVMknJ.png" alt="图片描述">  </p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录学习yolov5的原理，包括一些代码细节以及代码的复现。参考薛定谔的AI。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="pytorch" scheme="https://naughtyrabbit.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch学习" scheme="https://naughtyrabbit.github.io/tags/pytorch%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="https://naughtyrabbit.github.io/tags/yolo/"/>
    
  </entry>
  
  <entry>
    <title>yolov5目标检测原理探究（1）</title>
    <link href="https://naughtyrabbit.github.io/2022/01/26/pytorch-learning-adv-yolov5/"/>
    <id>https://naughtyrabbit.github.io/2022/01/26/pytorch-learning-adv-yolov5/</id>
    <published>2022-01-26T05:56:57.000Z</published>
    <updated>2022-03-23T04:31:00.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录学习yolov5w的原理，包括一些代码细节以及代码的复现。参考薛定谔的AI。</p></blockquote><span id="more"></span><h3 id="一、数据增强"><a href="#一、数据增强" class="headerlink" title="一、数据增强"></a>一、数据增强</h3><p>&amp;ensp;&amp;ensp;源码中位置：train.py-&gt;create_dataloader-&gt;LoadImagesAndLabels;<br>&amp;ensp;&amp;ensp;参数配置：data&#x2F;hyps&#x2F;hyp.scratch.yaml</p><pre><code>rectangular：同个batch里做rectangle宽高等比变换，加快训练hsv h：0.015#image HSV-Hue augmentation（fraction），色调hsv_s：0.7#image HSV-Saturation augmentation（fraction），饱和度hsv_v：0.4#image HSV-Value augmentation（fraction），曝光度degrees：0.0#image rotation（+/-deg），旋转translate：0.1#image translation（+/-fraction），平移scale：0.5#image scale（+/-gain），缩放shear：0.0#image shear（+/-deg），错切/非垂直投影perspective：0.0#image perspective（+/-fraction），range 0-0.001，透视变换flipud：0.0#image flip up-down（probability），上下翻转fliplr：0.5#image flip left-right（probability），左右翻转mosaic：1.0#image mosaic（probability），4图拼接mixup：0.0#image mixup（probability），图像互相融合copy_paste：0.0#segment copy-paste（probability），分割填补最后box坐标变换，segment坐标变换</code></pre><p><img src="https://s4.ax1x.com/2022/01/26/7LxxU0.png" alt="图片描述">  </p><h4 id="1-rectangular"><a href="#1-rectangular" class="headerlink" title="1. rectangular"></a>1. rectangular</h4><p>&amp;ensp;&amp;ensp;同个batch里做rectangle宽高等比变换，加快训练。使得每个batch的宽高一致，不同batch的宽高可以不一致，减少同一batch的图片中的“黑边”，同时这种客制化可以使得比如原来需要reshape到640x640的图片现在仅用reshape到540x450。这样输入的图片的尺寸减小，数据量大起来的话会减少非常可观的计算量。<br>&amp;ensp;&amp;ensp;源码位置：utils&#x2F;datasets.py Rectangular Training</p><h4 id="2-旋转、偏移、错切等"><a href="#2-旋转、偏移、错切等" class="headerlink" title="2. 旋转、偏移、错切等"></a>2. 旋转、偏移、错切等</h4><p>&amp;ensp;&amp;ensp;这几个操作都是把原图乘上一个变换矩阵，不同在于矩阵中参数设置。<br><img src="https://s4.ax1x.com/2022/01/26/7Opi7R.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;需要注意的是，所有的数据增强的操作对图片的操作都要在之后对相应标签也做操作。这部分源码参考：utils&#x2F;augmentations.py  </p><h3 id="二、backbone网络细节"><a href="#二、backbone网络细节" class="headerlink" title="二、backbone网络细节"></a>二、backbone网络细节</h3><p>&amp;ensp;&amp;ensp;可参考源码中models&#x2F;xxx.yaml文件。<br><img src="https://s4.ax1x.com/2022/01/27/7Xz2z8.png" alt="图片描述"><br><img src="https://s4.ax1x.com/2022/01/27/7jSxc8.png" alt="图片描述"> </p><h4 id="1-Conv层"><a href="#1-Conv层" class="headerlink" title="1.Conv层"></a>1.Conv层</h4><p><img src="https://s4.ax1x.com/2022/01/27/7jFOhj.png" alt="图片描述"> </p><h4 id="2-C3层"><a href="#2-C3层" class="headerlink" title="2.C3层"></a>2.C3层</h4><p>&amp;ensp;&amp;ensp;相比于BottleneckCSP的4个卷积少了一个，所以称为c3<br><img src="https://s4.ax1x.com/2022/01/27/7j60je.png" alt="图片描述"> </p><h4 id="3-SPPF层"><a href="#3-SPPF层" class="headerlink" title="3.SPPF层"></a>3.SPPF层</h4><p><img src="https://s4.ax1x.com/2022/01/27/7jc18f.png" alt="图片描述">   </p><h3 id="三、backbone网络源码解析"><a href="#三、backbone网络源码解析" class="headerlink" title="三、backbone网络源码解析"></a>三、backbone网络源码解析</h3><p><img src="https://s4.ax1x.com/2022/01/30/H90ktI.png" alt="图片描述"> </p><h3 id="四、边框预测细节"><a href="#四、边框预测细节" class="headerlink" title="四、边框预测细节"></a>四、边框预测细节</h3><p><img src="https://s4.ax1x.com/2022/02/02/HAsQoV.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;在yolov3&#x2F;2中，对正样本的处理是一个grid最多产生一个正样本，或者说如果某个区域存在一个目标，那么只有该区域中心点所落在的grid是正样本。这么做的坏处了正负样本的比例失衡，影响训练过程。yolov5把那个grid上下左右的grid都采样使用，因此范围用(0,1)无法表示。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录学习yolov5w的原理，包括一些代码细节以及代码的复现。参考薛定谔的AI。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="pytorch" scheme="https://naughtyrabbit.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch学习" scheme="https://naughtyrabbit.github.io/tags/pytorch%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="https://naughtyrabbit.github.io/tags/yolo/"/>
    
  </entry>
  
  <entry>
    <title>yolov3目标检测原理探究</title>
    <link href="https://naughtyrabbit.github.io/2022/01/19/pytorch-learning-adv-yolov3/"/>
    <id>https://naughtyrabbit.github.io/2022/01/19/pytorch-learning-adv-yolov3/</id>
    <published>2022-01-19T07:11:06.000Z</published>
    <updated>2022-02-27T02:18:30.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本篇主要记录学习yolov3的原理，主要是对yolov3论文的精读。参考：同济子豪兄</p></blockquote><span id="more"></span>  <h3 id="一、骨干网络-DarkNet53"><a href="#一、骨干网络-DarkNet53" class="headerlink" title="一、骨干网络 DarkNet53"></a>一、骨干网络 DarkNet53</h3><p>&amp;ensp;&amp;ensp;yolov3的骨干网络（backbone）从yolov2的darknet19改成了darknet53，如图：前面的1x&#x2F;2x代表重复这个block。输入可以是不同尺度的图像，但是必须是32的倍数，因为最后会对feature map进行32，16和8的下采样。然后输送给检测部分。<br><img src="https://s4.ax1x.com/2022/02/27/be5e3t.png" alt="图片描述">  </p><h3 id="二、网络结构"><a href="#二、网络结构" class="headerlink" title="二、网络结构"></a>二、网络结构</h3><pre><code>255 = 3 x 85(80 + 5)每个grid cell有三个anchor；每个anchor有(x, y, w, h, objectiveness)五个基本参数，加上coco数据集的80个类别的条件概率信息（在假设这个框的是物体的情况下，它是猫/狗/...的概率）。而前面则是grid cell的数量，13x13个，26x26个</code></pre><p><img src="https://s4.ax1x.com/2022/02/27/be51EQ.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;在yolov3中输出是三个尺度的，而显然，某个物体可能同时落在三个尺度的三个grid cell所产生的9个anchor，那此时，将由与ground truth的IOU最大的anchor去作拟合，这个anchor也是唯一的正样本。</p><h3 id="三、损失函数"><a href="#三、损失函数" class="headerlink" title="三、损失函数"></a>三、损失函数</h3><p>&amp;ensp;&amp;ensp;yolov3中，以阈值为区分，大于某个阈值的所有的anchor中与ground truth的IOU最大的那个作为正样本，其余不参与；小于某个阈值的则直接视为负样本。<br><img src="https://s4.ax1x.com/2022/02/27/be5dDU.png" alt="图片描述">  </p><pre><code>yolov3的损失函数分为3部分1. 正样本坐标的损失2. 正样本的置信度和类别3. 负样本的置信度</code></pre><p><img src="https://s4.ax1x.com/2022/02/27/be5r59.png" alt="图片描述">  </p><h3 id="四、yolov3论文精读"><a href="#四、yolov3论文精读" class="headerlink" title="四、yolov3论文精读"></a>四、yolov3论文精读</h3><p>&amp;ensp;&amp;ensp;前面也提到了。yolov3会把图像分成3个尺度，每个尺度的每一个grid cell会产生3个anchor，最后会选择这9个anchor中IOU最大的作为正样本，同时，把他的置信度设置为<strong>1</strong>，这一点是yolov3开始，因为在此前是把IOU作为置信度，但是实际效果可能是这个置信度最大才是0.7，显然，这样训练时的”激励“效果就不如把置信度设置为0要好，毕竟，这个anchor现在是唯一的正样本。另一方面，coco中的小目标IOU对像素偏移非常敏感，用IOU直接作为置信度可能达不到理想的效果。<br><img src="https://s4.ax1x.com/2022/02/27/be5WDO.png" alt="图片描述"><br>&amp;ensp;&amp;ensp;关于分类，yolov3也不再使用softmax，而是简单的对每一种类别进行二分类，因此，所有的概率和也不一定是1.<br><img src="https://pic.imgdb.cn/item/61eb96c82ab3f51d91b790ef.png" alt="图片描述">  </p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本篇主要记录学习yolov3的原理，主要是对yolov3论文的精读。参考：同济子豪兄&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="pytorch" scheme="https://naughtyrabbit.github.io/categories/pytorch/"/>
    
    
    <category term="pytorch学习" scheme="https://naughtyrabbit.github.io/tags/pytorch%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="yolo" scheme="https://naughtyrabbit.github.io/tags/yolo/"/>
    
  </entry>
  
</feed>
